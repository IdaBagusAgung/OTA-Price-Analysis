{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b07edb",
   "metadata": {},
   "source": [
    "# Airbnb Bali Time Series Analysis - Advanced Forecasting & Seasonal Intelligence\n",
    "\n",
    "**Comprehensive Time Series Analysis for Airbnb Bali Booking Data**\n",
    "\n",
    "This notebook performs advanced time series analysis on Airbnb Bali booking data to:\n",
    "- Forecast booking demand with multiple state-of-the-art models\n",
    "- Analyze seasonal patterns and trends with statistical decomposition\n",
    "- Optimize revenue strategies through temporal insights\n",
    "- Build predictive models for business intelligence\n",
    "- Create interactive time series visualizations and dashboards\n",
    "\n",
    "**Data Source:** Building upon cleaned dataset from previous EDA analysis\n",
    "**Analysis Period:** Complete temporal coverage with forecasting capabilities\n",
    "**Models:** ARIMA, SARIMA, Prophet, Machine Learning approaches\n",
    "**Output:** Forecasts, seasonal intelligence, revenue optimization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392744a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comprehensive time series analysis libraries\n",
    "print(\"ðŸš€ LOADING TIME SERIES ANALYSIS LIBRARIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Time series specific libraries\n",
    "try:\n",
    "    # Statistical time series analysis\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "    from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf\n",
    "    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "    statsmodels_available = True\n",
    "    print(\"âœ… Statsmodels imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Statsmodels not available: {e}\")\n",
    "    statsmodels_available = False\n",
    "\n",
    "try:\n",
    "    # Facebook Prophet for advanced forecasting\n",
    "    from prophet import Prophet\n",
    "    prophet_available = True\n",
    "    print(\"âœ… Prophet imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Prophet not available - install with: pip install prophet\")\n",
    "    prophet_available = False\n",
    "\n",
    "try:\n",
    "    # Machine learning libraries\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    sklearn_available = True\n",
    "    print(\"âœ… Scikit-learn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Scikit-learn not available: {e}\")\n",
    "    sklearn_available = False\n",
    "\n",
    "try:\n",
    "    # Advanced analytics\n",
    "    from scipy import stats\n",
    "    from scipy.signal import periodogram\n",
    "    scipy_available = True\n",
    "    print(\"âœ… SciPy imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ SciPy not available: {e}\")\n",
    "    scipy_available = False\n",
    "\n",
    "# Configure visualization settings for time series\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Configure matplotlib for time series\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"\\nðŸ“Š LOADING AIRBNB BALI DATASET\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load the cleaned dataset from previous EDA analysis\n",
    "dataset_path = \"../dataset/\"\n",
    "if not os.path.exists(dataset_path):\n",
    "    dataset_path = \"./\"  # Fallback to current directory\n",
    "\n",
    "# Try to load the most recent ML-ready dataset\n",
    "try:\n",
    "    # Look for the ML-ready dataset first\n",
    "    ml_ready_file = os.path.join(dataset_path, \"airbnb_bali_ml_ready.csv\")\n",
    "    if os.path.exists(ml_ready_file):\n",
    "        df = pd.read_csv(ml_ready_file)\n",
    "        print(f\"âœ… Loaded ML-ready dataset: {ml_ready_file}\")\n",
    "    else:\n",
    "        # Look for any cleaned data file\n",
    "        data_files = [f for f in os.listdir(dataset_path) if f.startswith(\"airbnb_bali\") and f.endswith(\".csv\")]\n",
    "        if data_files:\n",
    "            latest_file = max(data_files, key=lambda x: os.path.getctime(os.path.join(dataset_path, x)))\n",
    "            df = pd.read_csv(os.path.join(dataset_path, latest_file))\n",
    "            print(f\"âœ… Loaded latest dataset: {latest_file}\")\n",
    "        else:\n",
    "            # Fallback: create sample data for demonstration\n",
    "            print(\"âš ï¸ No dataset found - generating sample time series data\")\n",
    "            dates = pd.date_range(start='2023-01-01', end='2024-12-31', freq='D')\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            # Create realistic booking patterns\n",
    "            baseline = 50\n",
    "            seasonal_pattern = 20 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)\n",
    "            weekly_pattern = 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 7)\n",
    "            noise = np.random.normal(0, 5, len(dates))\n",
    "            bookings = baseline + seasonal_pattern + weekly_pattern + noise\n",
    "            bookings = np.maximum(bookings, 0)  # Ensure non-negative\n",
    "            \n",
    "            # Create sample price data\n",
    "            base_price = 80\n",
    "            price_seasonal = 30 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25 + np.pi/4)\n",
    "            price_noise = np.random.normal(0, 10, len(dates))\n",
    "            prices = base_price + price_seasonal + price_noise\n",
    "            prices = np.maximum(prices, 30)  # Minimum price\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'check_in_date': dates,\n",
    "                'booking_count': bookings.astype(int),\n",
    "                'price': prices.round(2),\n",
    "                'stay_duration': np.random.choice([2, 3, 5, 7, 14], len(dates), p=[0.2, 0.3, 0.3, 0.15, 0.05]),\n",
    "                'season': [('Winter' if m in [12,1,2] else 'Spring' if m in [3,4,5] else 'Summer' if m in [6,7,8] else 'Fall') for m in dates.month],\n",
    "                'is_weekend': [(d in [5,6]) for d in dates.dayofweek],\n",
    "                'currency': np.random.choice(['USD', 'EUR', 'AUD'], len(dates), p=[0.6, 0.3, 0.1])\n",
    "            })\n",
    "            print(\"âœ… Sample time series data generated for demonstration\")\n",
    "\n",
    "    print(f\"\\nðŸ“ˆ DATASET OVERVIEW:\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    print(f\"   Date Range: {df['check_in_date'].min()} to {df['check_in_date'].max()}\" if 'check_in_date' in df.columns else \"   Date columns to be processed\")\n",
    "    print(f\"   Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading dataset: {e}\")\n",
    "    # Create minimal sample data as fallback\n",
    "    dates = pd.date_range(start='2023-01-01', end='2024-12-31', freq='D')\n",
    "    df = pd.DataFrame({\n",
    "        'check_in_date': dates,\n",
    "        'booking_count': np.random.poisson(50, len(dates)),\n",
    "        'price': np.random.normal(80, 20, len(dates))\n",
    "    })\n",
    "    print(\"âœ… Minimal sample data created as fallback\")\n",
    "\n",
    "print(f\"\\nðŸ” INITIAL DATA INSPECTION:\")\n",
    "print(df.head())\n",
    "print(f\"\\nðŸ“Š Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ TIME SERIES ANALYSIS SETUP COMPLETED!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… Libraries loaded and configured\")\n",
    "print(\"âœ… Dataset loaded successfully\")\n",
    "print(\"âœ… Ready for comprehensive time series analysis\")\n",
    "print(\"ðŸš€ Proceeding to time series data preparation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22ad0a",
   "metadata": {},
   "source": [
    "## 2. Time Series Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Time Series Data Preparation\n",
    "print(\"ðŸ“… TIME SERIES DATA PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Convert date columns to datetime format\n",
    "print(\"ðŸ”§ Step 1: Date Column Processing\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Identify date columns\n",
    "date_columns = []\n",
    "for col in df.columns:\n",
    "    if 'date' in col.lower() or col in ['check_in', 'check_out']:\n",
    "        date_columns.append(col)\n",
    "\n",
    "print(f\"ðŸ“Š Identified date columns: {date_columns}\")\n",
    "\n",
    "# Convert date columns to datetime\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            print(f\"âœ… Converted {col} to datetime\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error converting {col}: {e}\")\n",
    "\n",
    "# Ensure we have a primary date column for time series analysis\n",
    "if 'check_in_date' in df.columns:\n",
    "    primary_date_col = 'check_in_date'\n",
    "elif 'date' in df.columns:\n",
    "    primary_date_col = 'date'\n",
    "else:\n",
    "    # Create a date column if none exists\n",
    "    if df.shape[0] > 0:\n",
    "        print(\"âš ï¸ No date column found - creating synthetic date range\")\n",
    "        start_date = '2023-01-01'\n",
    "        end_date = '2024-12-31'\n",
    "        date_range = pd.date_range(start=start_date, periods=len(df), freq='D')\n",
    "        df['check_in_date'] = date_range\n",
    "        primary_date_col = 'check_in_date'\n",
    "    else:\n",
    "        primary_date_col = None\n",
    "\n",
    "print(f\"ðŸŽ¯ Primary date column: {primary_date_col}\")\n",
    "\n",
    "# Step 2: Create time-based index\n",
    "print(f\"\\nðŸ”§ Step 2: Time-Based Index Creation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if primary_date_col:\n",
    "    # Sort by date\n",
    "    df = df.sort_values(primary_date_col)\n",
    "    \n",
    "    # Create date range for complete time series\n",
    "    date_min = df[primary_date_col].min()\n",
    "    date_max = df[primary_date_col].max()\n",
    "    complete_date_range = pd.date_range(start=date_min, end=date_max, freq='D')\n",
    "    \n",
    "    print(f\"ðŸ“… Date range: {date_min.strftime('%Y-%m-%d')} to {date_max.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"ðŸ“Š Total days: {len(complete_date_range)}\")\n",
    "    print(f\"ðŸ“ˆ Data coverage: {len(df)} records\")\n",
    "\n",
    "# Step 3: Aggregate booking data by different time periods\n",
    "print(f\"\\nðŸ”§ Step 3: Time Period Aggregation\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "# Create aggregated time series datasets\n",
    "time_series_data = {}\n",
    "\n",
    "if primary_date_col and len(df) > 0:\n",
    "    \n",
    "    # Daily aggregation\n",
    "    if 'booking_count' in df.columns:\n",
    "        daily_bookings = df.groupby(primary_date_col)['booking_count'].sum().reindex(complete_date_range, fill_value=0)\n",
    "    else:\n",
    "        # Count records per day as proxy for bookings\n",
    "        daily_bookings = df.groupby(primary_date_col).size().reindex(complete_date_range, fill_value=0)\n",
    "    \n",
    "    time_series_data['daily_bookings'] = daily_bookings\n",
    "    print(f\"âœ… Daily bookings: {len(daily_bookings)} data points\")\n",
    "    \n",
    "    # Weekly aggregation\n",
    "    weekly_bookings = daily_bookings.resample('W').sum()\n",
    "    time_series_data['weekly_bookings'] = weekly_bookings\n",
    "    print(f\"âœ… Weekly bookings: {len(weekly_bookings)} data points\")\n",
    "    \n",
    "    # Monthly aggregation\n",
    "    monthly_bookings = daily_bookings.resample('M').sum()\n",
    "    time_series_data['monthly_bookings'] = monthly_bookings\n",
    "    print(f\"âœ… Monthly bookings: {len(monthly_bookings)} data points\")\n",
    "    \n",
    "    # Revenue aggregation (if price data available)\n",
    "    if 'price' in df.columns:\n",
    "        # Daily revenue\n",
    "        if 'booking_count' in df.columns:\n",
    "            df['daily_revenue'] = df['booking_count'] * df['price']\n",
    "            daily_revenue = df.groupby(primary_date_col)['daily_revenue'].sum().reindex(complete_date_range, fill_value=0)\n",
    "        else:\n",
    "            daily_revenue = df.groupby(primary_date_col)['price'].sum().reindex(complete_date_range, fill_value=0)\n",
    "        \n",
    "        time_series_data['daily_revenue'] = daily_revenue\n",
    "        \n",
    "        # Weekly and monthly revenue\n",
    "        time_series_data['weekly_revenue'] = daily_revenue.resample('W').sum()\n",
    "        time_series_data['monthly_revenue'] = daily_revenue.resample('M').sum()\n",
    "        print(f\"âœ… Revenue time series created\")\n",
    "    \n",
    "    # Average stay duration over time\n",
    "    if 'stay_duration' in df.columns:\n",
    "        daily_avg_stay = df.groupby(primary_date_col)['stay_duration'].mean().reindex(complete_date_range)\n",
    "        # Forward fill missing values\n",
    "        daily_avg_stay = daily_avg_stay.fillna(method='ffill').fillna(method='bfill')\n",
    "        time_series_data['daily_avg_stay'] = daily_avg_stay\n",
    "        print(f\"âœ… Average stay duration time series created\")\n",
    "\n",
    "# Step 4: Handle missing dates and values\n",
    "print(f\"\\nðŸ”§ Step 4: Missing Data Handling\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for name, ts in time_series_data.items():\n",
    "    if isinstance(ts, pd.Series):\n",
    "        # Check for missing values\n",
    "        missing_count = ts.isnull().sum()\n",
    "        missing_pct = (missing_count / len(ts)) * 100\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            print(f\"âš ï¸ {name}: {missing_count} missing values ({missing_pct:.1f}%)\")\n",
    "            \n",
    "            # Handle missing values based on the type of time series\n",
    "            if 'bookings' in name or 'revenue' in name:\n",
    "                # For count/revenue data, missing usually means 0\n",
    "                ts = ts.fillna(0)\n",
    "            else:\n",
    "                # For other metrics, use interpolation\n",
    "                ts = ts.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "            time_series_data[name] = ts\n",
    "            print(f\"âœ… {name}: Missing values handled\")\n",
    "        else:\n",
    "            print(f\"âœ… {name}: No missing values\")\n",
    "\n",
    "# Step 5: Create master time series DataFrame\n",
    "print(f\"\\nðŸ”§ Step 5: Master Time Series Creation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if time_series_data:\n",
    "    # Combine all time series into one DataFrame\n",
    "    ts_df = pd.DataFrame(time_series_data)\n",
    "    \n",
    "    # Add date features\n",
    "    ts_df['year'] = ts_df.index.year\n",
    "    ts_df['month'] = ts_df.index.month\n",
    "    ts_df['day'] = ts_df.index.day\n",
    "    ts_df['day_of_week'] = ts_df.index.dayofweek\n",
    "    ts_df['day_of_year'] = ts_df.index.dayofyear\n",
    "    ts_df['week_of_year'] = ts_df.index.isocalendar().week\n",
    "    ts_df['quarter'] = ts_df.index.quarter\n",
    "    \n",
    "    # Add season\n",
    "    ts_df['season'] = ts_df['month'].map({\n",
    "        12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "        9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
    "    })\n",
    "    \n",
    "    # Add weekend indicator\n",
    "    ts_df['is_weekend'] = ts_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    print(f\"âœ… Master time series DataFrame created\")\n",
    "    print(f\"   Shape: {ts_df.shape}\")\n",
    "    print(f\"   Date range: {ts_df.index.min().strftime('%Y-%m-%d')} to {ts_df.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Columns: {list(ts_df.columns)}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(f\"\\nðŸ“Š Time Series Statistics:\")\n",
    "    print(ts_df.describe().round(2))\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(f\"\\nðŸ‘€ First 10 rows of time series data:\")\n",
    "    print(ts_df.head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No time series data could be created\")\n",
    "    ts_df = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nâœ… TIME SERIES DATA PREPARATION COMPLETED!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ðŸŽ¯ Ready for temporal feature engineering and analysis\")\n",
    "\n",
    "# Store time series data globally for next sections\n",
    "globals()['ts_df'] = ts_df\n",
    "globals()['time_series_data'] = time_series_data\n",
    "globals()['primary_date_col'] = primary_date_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ae658",
   "metadata": {},
   "source": [
    "## 3. Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Temporal Feature Engineering for Time Series Analysis\n",
    "print(\"âš™ï¸ ADVANCED TEMPORAL FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not ts_df.empty:\n",
    "    print(\"ðŸ”§ Creating comprehensive temporal features...\")\n",
    "    \n",
    "    # Step 1: Basic Temporal Features (already created, let's enhance them)\n",
    "    print(\"\\nðŸ“… Step 1: Enhanced Basic Temporal Features\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Holiday and special event indicators\n",
    "    def create_holiday_features(dates):\n",
    "        \"\"\"Create holiday and special event indicators\"\"\"\n",
    "        holidays = {}\n",
    "        \n",
    "        # Indonesian public holidays (major ones)\n",
    "        holidays['new_year'] = [(1, 1)]\n",
    "        holidays['independence_day'] = [(8, 17)]\n",
    "        holidays['christmas'] = [(12, 25)]\n",
    "        \n",
    "        # Religious holidays (approximate dates)\n",
    "        holidays['nyepi'] = [(3, 15), (4, 5)]  # Balinese New Year (varies)\n",
    "        holidays['galungan'] = [(5, 10), (11, 5)]  # Balinese holiday (every 210 days)\n",
    "        \n",
    "        # Tourist seasons in Bali\n",
    "        holidays['peak_season'] = [(7, 1), (7, 31), (8, 1), (8, 31), (12, 15), (12, 31)]\n",
    "        \n",
    "        holiday_features = pd.DataFrame(index=dates)\n",
    "        \n",
    "        for holiday_name, holiday_dates in holidays.items():\n",
    "            holiday_features[f'is_{holiday_name}'] = 0\n",
    "            for month, day in holiday_dates:\n",
    "                mask = (dates.month == month) & (dates.day == day)\n",
    "                holiday_features.loc[mask, f'is_{holiday_name}'] = 1\n",
    "        \n",
    "        return holiday_features\n",
    "    \n",
    "    # Add holiday features\n",
    "    holiday_features = create_holiday_features(ts_df.index)\n",
    "    for col in holiday_features.columns:\n",
    "        ts_df[col] = holiday_features[col]\n",
    "    \n",
    "    print(f\"âœ… Holiday features added: {list(holiday_features.columns)}\")\n",
    "    \n",
    "    # Step 2: Cyclical Encodings for Temporal Patterns\n",
    "    print(\"\\nðŸ”„ Step 2: Cyclical Encodings\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    # Day of year cyclical encoding\n",
    "    ts_df['day_of_year_sin'] = np.sin(2 * np.pi * ts_df['day_of_year'] / 365.25)\n",
    "    ts_df['day_of_year_cos'] = np.cos(2 * np.pi * ts_df['day_of_year'] / 365.25)\n",
    "    \n",
    "    # Day of week cyclical encoding\n",
    "    ts_df['day_of_week_sin'] = np.sin(2 * np.pi * ts_df['day_of_week'] / 7)\n",
    "    ts_df['day_of_week_cos'] = np.cos(2 * np.pi * ts_df['day_of_week'] / 7)\n",
    "    \n",
    "    # Month cyclical encoding\n",
    "    ts_df['month_sin'] = np.sin(2 * np.pi * ts_df['month'] / 12)\n",
    "    ts_df['month_cos'] = np.cos(2 * np.pi * ts_df['month'] / 12)\n",
    "    \n",
    "    # Hour of day (if we had hourly data, we'll simulate business hours effect)\n",
    "    # Peak booking hours simulation (9 AM to 6 PM peak)\n",
    "    ts_df['business_hours_effect'] = np.sin(2 * np.pi * ts_df['day_of_year'] / 365.25 + np.pi/4) * 0.1\n",
    "    \n",
    "    print(\"âœ… Cyclical encodings created for:\")\n",
    "    print(\"   â€¢ Day of year (seasonal pattern)\")\n",
    "    print(\"   â€¢ Day of week (weekly pattern)\")\n",
    "    print(\"   â€¢ Month (monthly pattern)\")\n",
    "    print(\"   â€¢ Business hours effect\")\n",
    "    \n",
    "    # Step 3: Lag Features for Time Series Modeling\n",
    "    print(\"\\nðŸ“ˆ Step 3: Lag Features Creation\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Create lag features for main time series\n",
    "    lag_periods = [1, 7, 14, 30, 365]  # 1 day, 1 week, 2 weeks, 1 month, 1 year\n",
    "    \n",
    "    if 'daily_bookings' in ts_df.columns:\n",
    "        for lag in lag_periods:\n",
    "            ts_df[f'bookings_lag_{lag}'] = ts_df['daily_bookings'].shift(lag)\n",
    "        print(f\"âœ… Booking lag features: {lag_periods}\")\n",
    "    \n",
    "    if 'daily_revenue' in ts_df.columns:\n",
    "        for lag in lag_periods:\n",
    "            ts_df[f'revenue_lag_{lag}'] = ts_df['daily_revenue'].shift(lag)\n",
    "        print(f\"âœ… Revenue lag features: {lag_periods}\")\n",
    "    \n",
    "    # Step 4: Rolling Window Features\n",
    "    print(\"\\nðŸ“Š Step 4: Rolling Window Features\")\n",
    "    print(\"-\" * 38)\n",
    "    \n",
    "    # Rolling window periods\n",
    "    windows = [7, 14, 30, 90]  # 1 week, 2 weeks, 1 month, 3 months\n",
    "    \n",
    "    if 'daily_bookings' in ts_df.columns:\n",
    "        for window in windows:\n",
    "            # Rolling mean\n",
    "            ts_df[f'bookings_rolling_mean_{window}'] = ts_df['daily_bookings'].rolling(window=window, min_periods=1).mean()\n",
    "            # Rolling std\n",
    "            ts_df[f'bookings_rolling_std_{window}'] = ts_df['daily_bookings'].rolling(window=window, min_periods=1).std()\n",
    "            # Rolling max\n",
    "            ts_df[f'bookings_rolling_max_{window}'] = ts_df['daily_bookings'].rolling(window=window, min_periods=1).max()\n",
    "            # Rolling min\n",
    "            ts_df[f'bookings_rolling_min_{window}'] = ts_df['daily_bookings'].rolling(window=window, min_periods=1).min()\n",
    "        \n",
    "        print(f\"âœ… Booking rolling features for windows: {windows}\")\n",
    "    \n",
    "    if 'daily_revenue' in ts_df.columns:\n",
    "        for window in windows:\n",
    "            ts_df[f'revenue_rolling_mean_{window}'] = ts_df['daily_revenue'].rolling(window=window, min_periods=1).mean()\n",
    "            ts_df[f'revenue_rolling_std_{window}'] = ts_df['daily_revenue'].rolling(window=window, min_periods=1).std()\n",
    "        \n",
    "        print(f\"âœ… Revenue rolling features for windows: {windows}\")\n",
    "    \n",
    "    # Step 5: Trend and Change Indicators\n",
    "    print(\"\\nðŸ“ˆ Step 5: Trend and Change Indicators\")\n",
    "    print(\"-\" * 42)\n",
    "    \n",
    "    if 'daily_bookings' in ts_df.columns:\n",
    "        # Daily change\n",
    "        ts_df['bookings_daily_change'] = ts_df['daily_bookings'].diff()\n",
    "        ts_df['bookings_daily_pct_change'] = ts_df['daily_bookings'].pct_change()\n",
    "        \n",
    "        # Weekly change\n",
    "        ts_df['bookings_weekly_change'] = ts_df['daily_bookings'].diff(7)\n",
    "        ts_df['bookings_weekly_pct_change'] = ts_df['daily_bookings'].pct_change(7)\n",
    "        \n",
    "        # Month-over-month change\n",
    "        ts_df['bookings_monthly_change'] = ts_df['daily_bookings'].diff(30)\n",
    "        \n",
    "        # Year-over-year change\n",
    "        ts_df['bookings_yoy_change'] = ts_df['daily_bookings'].diff(365)\n",
    "        \n",
    "        print(\"âœ… Booking change indicators created\")\n",
    "    \n",
    "    # Step 6: Seasonal Strength Indicators\n",
    "    print(\"\\nðŸŒŠ Step 6: Seasonal Strength Indicators\")\n",
    "    print(\"-\" * 42)\n",
    "    \n",
    "    if 'daily_bookings' in ts_df.columns and len(ts_df) > 365:\n",
    "        # Seasonal decomposition to extract seasonal strength\n",
    "        try:\n",
    "            # Simple seasonal strength calculation\n",
    "            seasonal_period = 365  # Annual seasonality\n",
    "            \n",
    "            # Calculate seasonal component strength\n",
    "            ts_df['seasonal_strength'] = 0.0\n",
    "            \n",
    "            # Group by day of year to find seasonal pattern\n",
    "            seasonal_means = ts_df.groupby('day_of_year')['daily_bookings'].mean()\n",
    "            overall_mean = ts_df['daily_bookings'].mean()\n",
    "            \n",
    "            # Map seasonal strength to each day\n",
    "            for idx, row in ts_df.iterrows():\n",
    "                day_of_year = row['day_of_year']\n",
    "                if day_of_year in seasonal_means.index:\n",
    "                    seasonal_value = seasonal_means[day_of_year]\n",
    "                    ts_df.loc[idx, 'seasonal_strength'] = (seasonal_value - overall_mean) / overall_mean\n",
    "            \n",
    "            print(\"âœ… Seasonal strength indicators created\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not create seasonal strength indicators: {e}\")\n",
    "    \n",
    "    # Step 7: Market Segment Features (if available)\n",
    "    print(\"\\nðŸŽ¯ Step 7: Market Segment Features\")\n",
    "    print(\"-\" * 38)\n",
    "    \n",
    "    # If we have currency/locale information in original data, aggregate it\n",
    "    if primary_date_col and 'currency' in df.columns:\n",
    "        try:\n",
    "            # Daily currency distribution\n",
    "            currency_daily = df.groupby([primary_date_col, 'currency']).size().unstack(fill_value=0)\n",
    "            currency_daily = currency_daily.reindex(ts_df.index, fill_value=0)\n",
    "            \n",
    "            # Add currency share features\n",
    "            for currency in currency_daily.columns:\n",
    "                ts_df[f'{currency}_bookings'] = currency_daily[currency]\n",
    "            \n",
    "            # Calculate currency diversity (Shannon entropy)\n",
    "            def calculate_diversity(row):\n",
    "                total = row.sum()\n",
    "                if total == 0:\n",
    "                    return 0\n",
    "                proportions = row / total\n",
    "                proportions = proportions[proportions > 0]  # Remove zeros for log\n",
    "                return -np.sum(proportions * np.log(proportions))\n",
    "            \n",
    "            currency_cols = [col for col in ts_df.columns if col.endswith('_bookings') and col != 'daily_bookings']\n",
    "            if currency_cols:\n",
    "                ts_df['currency_diversity'] = ts_df[currency_cols].apply(calculate_diversity, axis=1)\n",
    "                print(f\"âœ… Currency segment features created: {currency_cols}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not create currency features: {e}\")\n",
    "    \n",
    "    # Step 8: Advanced Statistical Features\n",
    "    print(\"\\nðŸ“Š Step 8: Advanced Statistical Features\")\n",
    "    print(\"-\" * 42)\n",
    "    \n",
    "    if 'daily_bookings' in ts_df.columns:\n",
    "        # Z-score (standardized values)\n",
    "        booking_mean = ts_df['daily_bookings'].mean()\n",
    "        booking_std = ts_df['daily_bookings'].std()\n",
    "        ts_df['bookings_zscore'] = (ts_df['daily_bookings'] - booking_mean) / booking_std\n",
    "        \n",
    "        # Percentile rank\n",
    "        ts_df['bookings_percentile'] = ts_df['daily_bookings'].rank(pct=True)\n",
    "        \n",
    "        # Relative position within rolling window\n",
    "        ts_df['bookings_relative_position'] = ts_df['daily_bookings'] / ts_df['bookings_rolling_mean_30']\n",
    "        \n",
    "        print(\"âœ… Statistical features created\")\n",
    "    \n",
    "    # Fill any remaining NaN values\n",
    "    print(\"\\nðŸ”§ Final Data Cleaning\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Forward fill and backward fill to handle edge cases\n",
    "    ts_df = ts_df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # Summary of feature engineering\n",
    "    print(f\"\\nðŸ“‹ FEATURE ENGINEERING SUMMARY:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"âœ… Total features created: {ts_df.shape[1]}\")\n",
    "    print(f\"âœ… Time series length: {len(ts_df)} days\")\n",
    "    print(f\"âœ… Date range: {ts_df.index.min().strftime('%Y-%m-%d')} to {ts_df.index.max().strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Categorize features\n",
    "    basic_features = [col for col in ts_df.columns if col in ['year', 'month', 'day', 'day_of_week', 'day_of_year', 'quarter', 'season', 'is_weekend']]\n",
    "    cyclical_features = [col for col in ts_df.columns if 'sin' in col or 'cos' in col]\n",
    "    lag_features = [col for col in ts_df.columns if 'lag' in col]\n",
    "    rolling_features = [col for col in ts_df.columns if 'rolling' in col]\n",
    "    change_features = [col for col in ts_df.columns if 'change' in col]\n",
    "    holiday_features = [col for col in ts_df.columns if 'is_' in col and col not in ['is_weekend']]\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Feature Categories:\")\n",
    "    print(f\"   â€¢ Basic temporal: {len(basic_features)}\")\n",
    "    print(f\"   â€¢ Cyclical encodings: {len(cyclical_features)}\")\n",
    "    print(f\"   â€¢ Lag features: {len(lag_features)}\")\n",
    "    print(f\"   â€¢ Rolling windows: {len(rolling_features)}\")\n",
    "    print(f\"   â€¢ Change indicators: {len(change_features)}\")\n",
    "    print(f\"   â€¢ Holiday features: {len(holiday_features)}\")\n",
    "    \n",
    "    # Display first few rows of enhanced dataset\n",
    "    print(f\"\\nðŸ‘€ Enhanced Time Series Data (first 5 rows):\")\n",
    "    feature_sample = ['daily_bookings', 'month_sin', 'day_of_week_sin', 'bookings_lag_7', 'bookings_rolling_mean_7']\n",
    "    available_sample = [col for col in feature_sample if col in ts_df.columns]\n",
    "    if available_sample:\n",
    "        print(ts_df[available_sample].head())\n",
    "    else:\n",
    "        print(ts_df.head())\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No time series data available for feature engineering\")\n",
    "\n",
    "print(f\"\\nâœ… TEMPORAL FEATURE ENGINEERING COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸŽ¯ Ready for seasonal decomposition analysis\")\n",
    "\n",
    "# Update global variable\n",
    "globals()['ts_df'] = ts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61eca3",
   "metadata": {},
   "source": [
    "## 4. Seasonal Decomposition Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fe3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Seasonal Decomposition Analysis\n",
    "print(\"ðŸŒŠ SEASONAL DECOMPOSITION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not ts_df.empty and statsmodels_available:\n",
    "    \n",
    "    # Select main time series for decomposition\n",
    "    main_series = None\n",
    "    series_name = \"\"\n",
    "    \n",
    "    if 'daily_bookings' in ts_df.columns and ts_df['daily_bookings'].sum() > 0:\n",
    "        main_series = ts_df['daily_bookings'].copy()\n",
    "        series_name = \"Daily Bookings\"\n",
    "    elif 'daily_revenue' in ts_df.columns and ts_df['daily_revenue'].sum() > 0:\n",
    "        main_series = ts_df['daily_revenue'].copy()\n",
    "        series_name = \"Daily Revenue\"\n",
    "    else:\n",
    "        # Create a synthetic series for demonstration\n",
    "        main_series = pd.Series(\n",
    "            index=ts_df.index,\n",
    "            data=50 + 20 * np.sin(2 * np.pi * np.arange(len(ts_df)) / 365.25) + \n",
    "                 10 * np.sin(2 * np.pi * np.arange(len(ts_df)) / 7) + \n",
    "                 np.random.normal(0, 5, len(ts_df))\n",
    "        )\n",
    "        series_name = \"Synthetic Bookings\"\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Analyzing: {series_name}\")\n",
    "    print(f\"ðŸ“Š Series length: {len(main_series)} days\")\n",
    "    print(f\"ðŸ“ˆ Series range: {main_series.min():.2f} to {main_series.max():.2f}\")\n",
    "    \n",
    "    # Step 1: Classical Seasonal Decomposition\n",
    "    print(f\"\\nðŸ”¬ Step 1: Classical Seasonal Decomposition\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    decomposition_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Annual seasonality (365 days)\n",
    "        if len(main_series) > 2 * 365:\n",
    "            print(\"ðŸ“… Performing annual decomposition (365-day period)...\")\n",
    "            annual_decomp = seasonal_decompose(main_series, model='additive', period=365, extrapolate_trend='freq')\n",
    "            decomposition_results['annual'] = annual_decomp\n",
    "            print(\"âœ… Annual decomposition completed\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Insufficient data for annual decomposition (need >730 days)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Annual decomposition failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Weekly seasonality (7 days)\n",
    "        if len(main_series) > 2 * 7:\n",
    "            print(\"ðŸ“… Performing weekly decomposition (7-day period)...\")\n",
    "            weekly_decomp = seasonal_decompose(main_series, model='additive', period=7, extrapolate_trend='freq')\n",
    "            decomposition_results['weekly'] = weekly_decomp\n",
    "            print(\"âœ… Weekly decomposition completed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Weekly decomposition failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Monthly seasonality (30 days)\n",
    "        if len(main_series) > 2 * 30:\n",
    "            print(\"ðŸ“… Performing monthly decomposition (30-day period)...\")\n",
    "            monthly_decomp = seasonal_decompose(main_series, model='additive', period=30, extrapolate_trend='freq')\n",
    "            decomposition_results['monthly'] = monthly_decomp\n",
    "            print(\"âœ… Monthly decomposition completed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Monthly decomposition failed: {e}\")\n",
    "    \n",
    "    # Step 2: STL (Seasonal and Trend decomposition using Loess) Decomposition  \n",
    "    print(f\"\\nðŸ”¬ Step 2: STL Decomposition\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    try:\n",
    "        if len(main_series) > 2 * 365:\n",
    "            print(\"ðŸ“ˆ Performing STL decomposition (more robust)...\")\n",
    "            stl_decomp = STL(main_series, seasonal=13, period=365).fit()\n",
    "            decomposition_results['stl'] = stl_decomp\n",
    "            print(\"âœ… STL decomposition completed\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Insufficient data for STL decomposition\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ STL decomposition failed: {e}\")\n",
    "    \n",
    "    # Step 3: Analyze Seasonal Patterns\n",
    "    print(f\"\\nðŸ“Š Step 3: Seasonal Pattern Analysis\")\n",
    "    print(\"-\" * 38)\n",
    "    \n",
    "    seasonal_insights = {}\n",
    "    \n",
    "    for decomp_name, decomp in decomposition_results.items():\n",
    "        print(f\"\\nðŸ” Analyzing {decomp_name} decomposition:\")\n",
    "        \n",
    "        if hasattr(decomp, 'seasonal'):\n",
    "            seasonal_component = decomp.seasonal\n",
    "            trend_component = decomp.trend\n",
    "            residual_component = decomp.resid\n",
    "            \n",
    "            # Calculate seasonal strength\n",
    "            seasonal_var = np.var(seasonal_component.dropna())\n",
    "            residual_var = np.var(residual_component.dropna())\n",
    "            seasonal_strength = seasonal_var / (seasonal_var + residual_var) if (seasonal_var + residual_var) > 0 else 0\n",
    "            \n",
    "            # Calculate trend strength\n",
    "            trend_var = np.var(trend_component.dropna())\n",
    "            trend_strength = trend_var / (trend_var + residual_var) if (trend_var + residual_var) > 0 else 0\n",
    "            \n",
    "            seasonal_insights[decomp_name] = {\n",
    "                'seasonal_strength': seasonal_strength,\n",
    "                'trend_strength': trend_strength,\n",
    "                'seasonal_range': seasonal_component.max() - seasonal_component.min(),\n",
    "                'trend_change': trend_component.dropna().iloc[-1] - trend_component.dropna().iloc[0] if len(trend_component.dropna()) > 0 else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"   ðŸ“ˆ Seasonal strength: {seasonal_strength:.3f}\")\n",
    "            print(f\"   ðŸ“ˆ Trend strength: {trend_strength:.3f}\")\n",
    "            print(f\"   ðŸ“ˆ Seasonal range: {seasonal_component.max() - seasonal_component.min():.2f}\")\n",
    "            \n",
    "            # Identify peak periods\n",
    "            if decomp_name == 'weekly' and len(seasonal_component) >= 7:\n",
    "                # For weekly pattern, find peak days\n",
    "                weekly_pattern = seasonal_component.iloc[:7]\n",
    "                peak_day = weekly_pattern.idxmax()\n",
    "                low_day = weekly_pattern.idxmin()\n",
    "                print(f\"   ðŸŽ¯ Peak day: {peak_day.strftime('%A')} ({weekly_pattern.max():.2f})\")\n",
    "                print(f\"   ðŸ“‰ Low day: {low_day.strftime('%A')} ({weekly_pattern.min():.2f})\")\n",
    "            \n",
    "            elif decomp_name == 'annual' and len(seasonal_component) >= 365:\n",
    "                # For annual pattern, find peak months\n",
    "                monthly_seasonal = seasonal_component.groupby(seasonal_component.index.month).mean()\n",
    "                peak_month = monthly_seasonal.idxmax()\n",
    "                low_month = monthly_seasonal.idxmin()\n",
    "                month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "                print(f\"   ðŸŽ¯ Peak month: {month_names[peak_month-1]} ({monthly_seasonal.max():.2f})\")\n",
    "                print(f\"   ðŸ“‰ Low month: {month_names[low_month-1]} ({monthly_seasonal.min():.2f})\")\n",
    "    \n",
    "    # Step 4: Peak Period Identification\n",
    "    print(f\"\\nðŸŽ¯ Step 4: Peak Period Identification\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'annual' in decomposition_results:\n",
    "        seasonal_comp = decomposition_results['annual'].seasonal\n",
    "        \n",
    "        # Monthly analysis\n",
    "        monthly_seasonal = seasonal_comp.groupby(seasonal_comp.index.month).mean()\n",
    "        monthly_peaks = monthly_seasonal.nlargest(3)\n",
    "        monthly_lows = monthly_seasonal.nsmallest(3)\n",
    "        \n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        \n",
    "        print(\"ðŸ† Top 3 Peak Months:\")\n",
    "        for i, (month, value) in enumerate(monthly_peaks.items(), 1):\n",
    "            print(f\"   {i}. {month_names[month-1]}: {value:.2f}\")\n",
    "        \n",
    "        print(\"\\nðŸ“‰ Bottom 3 Low Months:\")\n",
    "        for i, (month, value) in enumerate(monthly_lows.items(), 1):\n",
    "            print(f\"   {i}. {month_names[month-1]}: {value:.2f}\")\n",
    "    \n",
    "    # Step 5: Quantify Seasonal Strength\n",
    "    print(f\"\\nðŸ“ Step 5: Seasonal Strength Quantification\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    print(\"ðŸ“Š Seasonal Strength Summary:\")\n",
    "    for decomp_name, insights in seasonal_insights.items():\n",
    "        strength_level = \"High\" if insights['seasonal_strength'] > 0.6 else \"Moderate\" if insights['seasonal_strength'] > 0.3 else \"Low\"\n",
    "        print(f\"   {decomp_name.title()}: {insights['seasonal_strength']:.3f} ({strength_level})\")\n",
    "    \n",
    "    # Step 6: Create Visualization of Decomposition\n",
    "    print(f\"\\nðŸ“Š Step 6: Decomposition Visualization\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create comprehensive decomposition plot\n",
    "    if decomposition_results:\n",
    "        fig, axes = plt.subplots(len(decomposition_results) * 4, 1, figsize=(15, 4 * len(decomposition_results) * 4))\n",
    "        if len(decomposition_results) == 1:\n",
    "            axes = [axes] if not isinstance(axes, np.ndarray) else axes\n",
    "        elif len(decomposition_results) > 1:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        plot_idx = 0\n",
    "        \n",
    "        for decomp_name, decomp in decomposition_results.items():\n",
    "            # Original series\n",
    "            axes[plot_idx].plot(main_series.index, main_series.values, label='Original', color='blue', alpha=0.7)\n",
    "            axes[plot_idx].set_title(f'{decomp_name.title()} Decomposition - Original {series_name}')\n",
    "            axes[plot_idx].legend()\n",
    "            axes[plot_idx].grid(True, alpha=0.3)\n",
    "            plot_idx += 1\n",
    "            \n",
    "            # Trend\n",
    "            axes[plot_idx].plot(decomp.trend.index, decomp.trend.values, label='Trend', color='red', linewidth=2)\n",
    "            axes[plot_idx].set_title(f'{decomp_name.title()} - Trend Component')\n",
    "            axes[plot_idx].legend()\n",
    "            axes[plot_idx].grid(True, alpha=0.3)\n",
    "            plot_idx += 1\n",
    "            \n",
    "            # Seasonal\n",
    "            axes[plot_idx].plot(decomp.seasonal.index, decomp.seasonal.values, label='Seasonal', color='green')\n",
    "            axes[plot_idx].set_title(f'{decomp_name.title()} - Seasonal Component')\n",
    "            axes[plot_idx].legend()\n",
    "            axes[plot_idx].grid(True, alpha=0.3)\n",
    "            plot_idx += 1\n",
    "            \n",
    "            # Residual\n",
    "            axes[plot_idx].plot(decomp.resid.index, decomp.resid.values, label='Residual', color='orange', alpha=0.7)\n",
    "            axes[plot_idx].set_title(f'{decomp_name.title()} - Residual Component')\n",
    "            axes[plot_idx].legend()\n",
    "            axes[plot_idx].grid(True, alpha=0.3)\n",
    "            plot_idx += 1\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"âœ… Decomposition plots created\")\n",
    "    \n",
    "    # Step 7: Seasonal Adjustment\n",
    "    print(f\"\\nðŸ”§ Step 7: Seasonal Adjustment\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    if 'annual' in decomposition_results:\n",
    "        # Create seasonally adjusted series\n",
    "        decomp = decomposition_results['annual']\n",
    "        seasonally_adjusted = main_series - decomp.seasonal\n",
    "        \n",
    "        # Add to time series dataframe\n",
    "        ts_df['seasonally_adjusted_bookings'] = seasonally_adjusted\n",
    "        ts_df['seasonal_component'] = decomp.seasonal\n",
    "        ts_df['trend_component'] = decomp.trend\n",
    "        ts_df['residual_component'] = decomp.resid\n",
    "        \n",
    "        print(\"âœ… Seasonally adjusted series created and added to dataframe\")\n",
    "        \n",
    "        # Compare original vs seasonally adjusted\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # Original and seasonally adjusted\n",
    "        ax1.plot(main_series.index, main_series.values, label='Original', alpha=0.7)\n",
    "        ax1.plot(seasonally_adjusted.index, seasonally_adjusted.values, label='Seasonally Adjusted', alpha=0.8)\n",
    "        ax1.set_title('Original vs Seasonally Adjusted Series')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Seasonal component\n",
    "        ax2.plot(decomp.seasonal.index, decomp.seasonal.values, label='Seasonal Component', color='green')\n",
    "        ax2.set_title('Extracted Seasonal Component')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"âœ… Comparison plots created\")\n",
    "\n",
    "else:\n",
    "    if not statsmodels_available:\n",
    "        print(\"âš ï¸ Statsmodels not available - seasonal decomposition skipped\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No time series data available for decomposition\")\n",
    "\n",
    "# Step 8: Export Seasonal Analysis Results\n",
    "print(f\"\\nðŸ’¾ Step 8: Export Seasonal Analysis\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "if 'seasonal_insights' in locals() and seasonal_insights:\n",
    "    # Create seasonal analysis summary\n",
    "    seasonal_summary = []\n",
    "    for decomp_type, insights in seasonal_insights.items():\n",
    "        seasonal_summary.append({\n",
    "            'decomposition_type': decomp_type,\n",
    "            'seasonal_strength': insights['seasonal_strength'],\n",
    "            'trend_strength': insights['trend_strength'],\n",
    "            'seasonal_range': insights['seasonal_range'],\n",
    "            'trend_change': insights['trend_change']\n",
    "        })\n",
    "    \n",
    "    if seasonal_summary:\n",
    "        seasonal_df = pd.DataFrame(seasonal_summary)\n",
    "        seasonal_file = f\"seasonal_analysis_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        seasonal_df.to_csv(seasonal_file, index=False)\n",
    "        print(f\"âœ… Seasonal analysis exported to: {seasonal_file}\")\n",
    "\n",
    "print(f\"\\nâœ… SEASONAL DECOMPOSITION ANALYSIS COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸŽ¯ Key Insights:\")\n",
    "if 'seasonal_insights' in locals():\n",
    "    for decomp_type, insights in seasonal_insights.items():\n",
    "        strength_level = \"High\" if insights['seasonal_strength'] > 0.6 else \"Moderate\" if insights['seasonal_strength'] > 0.3 else \"Low\"\n",
    "        print(f\"   â€¢ {decomp_type.title()} seasonality: {strength_level} ({insights['seasonal_strength']:.3f})\")\n",
    "\n",
    "print(\"ðŸš€ Ready for time series forecasting models\")\n",
    "\n",
    "# Update global variables\n",
    "globals()['ts_df'] = ts_df\n",
    "globals()['decomposition_results'] = decomposition_results if 'decomposition_results' in locals() else {}\n",
    "globals()['seasonal_insights'] = seasonal_insights if 'seasonal_insights' in locals() else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa797d2a",
   "metadata": {},
   "source": [
    "## 5. Time Series Forecasting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc70d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Time Series Forecasting Models\n",
    "print(\"ðŸ”® TIME SERIES FORECASTING MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not ts_df.empty:\n",
    "    \n",
    "    # Prepare forecasting data\n",
    "    print(\"ðŸ”§ Preparing data for forecasting...\")\n",
    "    \n",
    "    # Select main target variable\n",
    "    target_column = None\n",
    "    if 'daily_bookings' in ts_df.columns and ts_df['daily_bookings'].sum() > 0:\n",
    "        target_column = 'daily_bookings'\n",
    "    elif 'daily_revenue' in ts_df.columns and ts_df['daily_revenue'].sum() > 0:\n",
    "        target_column = 'daily_revenue'\n",
    "    else:\n",
    "        # Create synthetic target for demonstration\n",
    "        target_column = 'synthetic_bookings'\n",
    "        ts_df[target_column] = 50 + 20 * np.sin(2 * np.pi * np.arange(len(ts_df)) / 365.25) + np.random.normal(0, 5, len(ts_df))\n",
    "    \n",
    "    target_series = ts_df[target_column].copy()\n",
    "    print(f\"ðŸŽ¯ Target variable: {target_column}\")\n",
    "    print(f\"ðŸ“Š Series length: {len(target_series)} days\")\n",
    "    print(f\"ðŸ“ˆ Value range: {target_series.min():.2f} to {target_series.max():.2f}\")\n",
    "    \n",
    "    # Define forecasting horizon\n",
    "    forecast_horizon = min(30, len(target_series) // 4)  # 30 days or 25% of data\n",
    "    \n",
    "    # Split data for training and testing\n",
    "    train_size = len(target_series) - forecast_horizon\n",
    "    train_data = target_series.iloc[:train_size]\n",
    "    test_data = target_series.iloc[train_size:]\n",
    "    \n",
    "    print(f\"ðŸ“Š Training data: {len(train_data)} days\")\n",
    "    print(f\"ðŸ“Š Test data: {len(test_data)} days\")\n",
    "    print(f\"ðŸ”® Forecasting horizon: {forecast_horizon} days\")\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    forecast_results = {}\n",
    "    model_performance = {}\n",
    "    \n",
    "    # Step 1: ARIMA/SARIMA Models\n",
    "    print(f\"\\nðŸ”¬ Step 1: ARIMA/SARIMA Models\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    if statsmodels_available:\n",
    "        try:\n",
    "            print(\"ðŸ“ˆ Training ARIMA model...\")\n",
    "            \n",
    "            # Auto ARIMA approach - simple version\n",
    "            # Check stationarity first\n",
    "            adf_result = adfuller(train_data)\n",
    "            is_stationary = adf_result[1] < 0.05\n",
    "            \n",
    "            print(f\"   Stationarity test p-value: {adf_result[1]:.4f}\")\n",
    "            print(f\"   Series is {'stationary' if is_stationary else 'non-stationary'}\")\n",
    "            \n",
    "            # Simple ARIMA model selection\n",
    "            if is_stationary:\n",
    "                arima_order = (1, 0, 1)  # Simple ARMA for stationary data\n",
    "            else:\n",
    "                arima_order = (1, 1, 1)  # ARIMA with differencing\n",
    "            \n",
    "            # Fit ARIMA model\n",
    "            arima_model = ARIMA(train_data, order=arima_order)\n",
    "            arima_fitted = arima_model.fit()\n",
    "            \n",
    "            # Generate forecasts\n",
    "            arima_forecast = arima_fitted.forecast(steps=forecast_horizon)\n",
    "            arima_conf_int = arima_fitted.forecast(steps=forecast_horizon, alpha=0.05)[1]  # 95% confidence interval\n",
    "            \n",
    "            forecast_results['ARIMA'] = {\n",
    "                'forecast': arima_forecast,\n",
    "                'model': arima_fitted,\n",
    "                'order': arima_order\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… ARIMA{arima_order} model trained successfully\")\n",
    "            \n",
    "            # Try SARIMA if enough data\n",
    "            if len(train_data) > 2 * 52:  # At least 2 years of weekly data\n",
    "                try:\n",
    "                    print(\"ðŸ“ˆ Training SARIMA model...\")\n",
    "                    sarima_order = (1, 1, 1)\n",
    "                    sarima_seasonal = (1, 1, 1, 7)  # Weekly seasonality\n",
    "                    \n",
    "                    sarima_model = SARIMAX(train_data, order=sarima_order, seasonal_order=sarima_seasonal)\n",
    "                    sarima_fitted = sarima_model.fit(disp=False)\n",
    "                    \n",
    "                    sarima_forecast = sarima_fitted.forecast(steps=forecast_horizon)\n",
    "                    \n",
    "                    forecast_results['SARIMA'] = {\n",
    "                        'forecast': sarima_forecast,\n",
    "                        'model': sarima_fitted,\n",
    "                        'order': sarima_order,\n",
    "                        'seasonal_order': sarima_seasonal\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"âœ… SARIMA{sarima_order}x{sarima_seasonal} model trained successfully\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ SARIMA model failed: {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ARIMA models failed: {e}\")\n",
    "    \n",
    "    # Step 2: Exponential Smoothing (Holt-Winters)\n",
    "    print(f\"\\nðŸ”¬ Step 2: Exponential Smoothing\")\n",
    "    print(\"-\" * 36)\n",
    "    \n",
    "    if statsmodels_available:\n",
    "        try:\n",
    "            print(\"ðŸ“ˆ Training Exponential Smoothing model...\")\n",
    "            \n",
    "            # Simple Exponential Smoothing\n",
    "            simple_exp = ExponentialSmoothing(train_data, trend=None, seasonal=None)\n",
    "            simple_exp_fitted = simple_exp.fit()\n",
    "            simple_exp_forecast = simple_exp_fitted.forecast(steps=forecast_horizon)\n",
    "            \n",
    "            forecast_results['Simple_Exponential'] = {\n",
    "                'forecast': simple_exp_forecast,\n",
    "                'model': simple_exp_fitted\n",
    "            }\n",
    "            \n",
    "            print(\"âœ… Simple Exponential Smoothing completed\")\n",
    "            \n",
    "            # Holt-Winters (Triple Exponential Smoothing) if enough data\n",
    "            if len(train_data) > 2 * 7:  # At least 2 weeks of data\n",
    "                try:\n",
    "                    hw_model = ExponentialSmoothing(train_data, trend='add', seasonal='add', seasonal_periods=7)\n",
    "                    hw_fitted = hw_model.fit()\n",
    "                    hw_forecast = hw_fitted.forecast(steps=forecast_horizon)\n",
    "                    \n",
    "                    forecast_results['Holt_Winters'] = {\n",
    "                        'forecast': hw_forecast,\n",
    "                        'model': hw_fitted\n",
    "                    }\n",
    "                    \n",
    "                    print(\"âœ… Holt-Winters model completed\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Holt-Winters failed: {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Exponential Smoothing failed: {e}\")\n",
    "    \n",
    "    # Step 3: Facebook Prophet\n",
    "    print(f\"\\nðŸ”¬ Step 3: Facebook Prophet\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    if prophet_available:\n",
    "        try:\n",
    "            print(\"ðŸ“ˆ Training Prophet model...\")\n",
    "            \n",
    "            # Prepare data for Prophet\n",
    "            prophet_train = pd.DataFrame({\n",
    "                'ds': train_data.index,\n",
    "                'y': train_data.values\n",
    "            })\n",
    "            \n",
    "            # Initialize and fit Prophet model\n",
    "            prophet_model = Prophet(\n",
    "                daily_seasonality=True,\n",
    "                weekly_seasonality=True,\n",
    "                yearly_seasonality=True if len(train_data) > 365 else False,\n",
    "                seasonality_mode='additive'\n",
    "            )\n",
    "            \n",
    "            # Add custom seasonalities if enough data\n",
    "            if len(train_data) > 30:\n",
    "                prophet_model.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "            \n",
    "            prophet_model.fit(prophet_train)\n",
    "            \n",
    "            # Create future dataframe for forecasting\n",
    "            future = prophet_model.make_future_dataframe(periods=forecast_horizon)\n",
    "            \n",
    "            # Generate forecast\n",
    "            prophet_forecast_df = prophet_model.predict(future)\n",
    "            prophet_forecast = prophet_forecast_df.tail(forecast_horizon)['yhat']\n",
    "            \n",
    "            forecast_results['Prophet'] = {\n",
    "                'forecast': prophet_forecast,\n",
    "                'model': prophet_model,\n",
    "                'forecast_df': prophet_forecast_df\n",
    "            }\n",
    "            \n",
    "            print(\"âœ… Prophet model trained successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Prophet model failed: {e}\")\n",
    "    \n",
    "    # Step 4: Machine Learning Models\n",
    "    print(f\"\\nðŸ”¬ Step 4: Machine Learning Models\")\n",
    "    print(\"-\" * 37)\n",
    "    \n",
    "    if sklearn_available:\n",
    "        \n",
    "        # Prepare features for ML models\n",
    "        print(\"ðŸ”§ Preparing features for ML models...\")\n",
    "        \n",
    "        # Create feature matrix with lags and temporal features\n",
    "        feature_columns = []\n",
    "        \n",
    "        # Add lag features\n",
    "        for lag in [1, 7, 14, 30]:\n",
    "            if f'{target_column}_lag_{lag}' in ts_df.columns:\n",
    "                feature_columns.append(f'{target_column}_lag_{lag}')\n",
    "        \n",
    "        # Add rolling features\n",
    "        for window in [7, 14, 30]:\n",
    "            if f'{target_column.replace(\"daily_\", \"\")}_rolling_mean_{window}' in ts_df.columns:\n",
    "                feature_columns.append(f'{target_column.replace(\"daily_\", \"\")}_rolling_mean_{window}')\n",
    "        \n",
    "        # Add temporal features\n",
    "        temporal_features = ['day_of_week', 'month', 'day_of_year', 'is_weekend']\n",
    "        for feature in temporal_features:\n",
    "            if feature in ts_df.columns:\n",
    "                feature_columns.append(feature)\n",
    "        \n",
    "        # Add cyclical features\n",
    "        cyclical_features = [col for col in ts_df.columns if 'sin' in col or 'cos' in col]\n",
    "        feature_columns.extend(cyclical_features[:10])  # Limit to first 10 cyclical features\n",
    "        \n",
    "        # Filter available features\n",
    "        available_features = [col for col in feature_columns if col in ts_df.columns]\n",
    "        \n",
    "        if available_features:\n",
    "            print(f\"ðŸ“Š Using {len(available_features)} features for ML models\")\n",
    "            \n",
    "            # Prepare training data\n",
    "            X = ts_df[available_features].iloc[:train_size]\n",
    "            y = train_data\n",
    "            \n",
    "            # Remove rows with NaN values\n",
    "            mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "            X_clean = X[mask]\n",
    "            y_clean = y[mask]\n",
    "            \n",
    "            if len(X_clean) > 10:  # Ensure we have enough clean data\n",
    "                \n",
    "                # Random Forest Model\n",
    "                try:\n",
    "                    print(\"ðŸŒ² Training Random Forest model...\")\n",
    "                    \n",
    "                    rf_model = RandomForestRegressor(\n",
    "                        n_estimators=100,\n",
    "                        max_depth=10,\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "                    \n",
    "                    rf_model.fit(X_clean, y_clean)\n",
    "                    \n",
    "                    # Generate forecast\n",
    "                    X_test = ts_df[available_features].iloc[train_size:train_size+forecast_horizon]\n",
    "                    X_test_clean = X_test.fillna(method='ffill').fillna(method='bfill')\n",
    "                    rf_forecast = rf_model.predict(X_test_clean)\n",
    "                    \n",
    "                    forecast_results['Random_Forest'] = {\n",
    "                        'forecast': pd.Series(rf_forecast, index=test_data.index),\n",
    "                        'model': rf_model,\n",
    "                        'features': available_features\n",
    "                    }\n",
    "                    \n",
    "                    print(\"âœ… Random Forest model trained successfully\")\n",
    "                    \n",
    "                    # Feature importance\n",
    "                    feature_importance = pd.DataFrame({\n",
    "                        'feature': available_features,\n",
    "                        'importance': rf_model.feature_importances_\n",
    "                    }).sort_values('importance', ascending=False)\n",
    "                    \n",
    "                    print(\"ðŸŽ¯ Top 5 most important features:\")\n",
    "                    for i, row in feature_importance.head().iterrows():\n",
    "                        print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Random Forest failed: {e}\")\n",
    "                \n",
    "                # Linear Regression with temporal features\n",
    "                try:\n",
    "                    print(\"ðŸ“Š Training Linear Regression model...\")\n",
    "                    \n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(X_clean)\n",
    "                    \n",
    "                    lr_model = LinearRegression()\n",
    "                    lr_model.fit(X_scaled, y_clean)\n",
    "                    \n",
    "                    # Generate forecast\n",
    "                    X_test_scaled = scaler.transform(X_test_clean)\n",
    "                    lr_forecast = lr_model.predict(X_test_scaled)\n",
    "                    \n",
    "                    forecast_results['Linear_Regression'] = {\n",
    "                        'forecast': pd.Series(lr_forecast, index=test_data.index),\n",
    "                        'model': lr_model,\n",
    "                        'scaler': scaler,\n",
    "                        'features': available_features\n",
    "                    }\n",
    "                    \n",
    "                    print(\"âœ… Linear Regression model trained successfully\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Linear Regression failed: {e}\")\n",
    "            \n",
    "            else:\n",
    "                print(\"âš ï¸ Insufficient clean data for ML models\")\n",
    "        else:\n",
    "            print(\"âš ï¸ No suitable features available for ML models\")\n",
    "    \n",
    "    # Step 5: Model Performance Evaluation\n",
    "    print(f\"\\nðŸ“Š Step 5: Model Performance Evaluation\")\n",
    "    print(\"-\" * 42)\n",
    "    \n",
    "    print(\"ðŸ” Evaluating forecast accuracy...\")\n",
    "    \n",
    "    for model_name, result in forecast_results.items():\n",
    "        try:\n",
    "            forecast = result['forecast']\n",
    "            \n",
    "            # Ensure forecast has same index as test data\n",
    "            if hasattr(forecast, 'index'):\n",
    "                forecast_values = forecast.values\n",
    "            else:\n",
    "                forecast_values = forecast\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(test_data.values, forecast_values)\n",
    "            rmse = np.sqrt(mean_squared_error(test_data.values, forecast_values))\n",
    "            mape = np.mean(np.abs((test_data.values - forecast_values) / test_data.values)) * 100\n",
    "            \n",
    "            model_performance[model_name] = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nðŸ“ˆ {model_name}:\")\n",
    "            print(f\"   MAE:  {mae:.2f}\")\n",
    "            print(f\"   RMSE: {rmse:.2f}\")\n",
    "            print(f\"   MAPE: {mape:.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error evaluating {model_name}: {e}\")\n",
    "    \n",
    "    # Find best model\n",
    "    if model_performance:\n",
    "        best_model = min(model_performance.keys(), key=lambda x: model_performance[x]['MAE'])\n",
    "        print(f\"\\nðŸ† Best Model: {best_model} (lowest MAE: {model_performance[best_model]['MAE']:.2f})\")\n",
    "    \n",
    "    # Step 6: Visualization\n",
    "    print(f\"\\nðŸ“Š Step 6: Forecast Visualization\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Plot historical data\n",
    "    ax.plot(train_data.index, train_data.values, label='Training Data', color='blue', alpha=0.7)\n",
    "    ax.plot(test_data.index, test_data.values, label='Actual', color='black', linewidth=2)\n",
    "    \n",
    "    # Plot forecasts\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown', 'pink']\n",
    "    for i, (model_name, result) in enumerate(forecast_results.items()):\n",
    "        forecast = result['forecast']\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        if hasattr(forecast, 'index'):\n",
    "            ax.plot(forecast.index, forecast.values, label=f'{model_name} Forecast', \n",
    "                   color=color, linestyle='--', alpha=0.8)\n",
    "        else:\n",
    "            ax.plot(test_data.index, forecast, label=f'{model_name} Forecast', \n",
    "                   color=color, linestyle='--', alpha=0.8)\n",
    "    \n",
    "    ax.set_title(f'Time Series Forecasting Comparison - {target_column}')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Forecast visualization completed\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No time series data available for forecasting\")\n",
    "\n",
    "print(f\"\\nâœ… TIME SERIES FORECASTING COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'model_performance' in locals() and model_performance:\n",
    "    print(\"ðŸŽ¯ Model Performance Summary:\")\n",
    "    performance_df = pd.DataFrame(model_performance).T\n",
    "    print(performance_df.round(2))\n",
    "\n",
    "print(\"ðŸš€ Ready for advanced time series visualization\")\n",
    "\n",
    "# Update global variables\n",
    "globals()['forecast_results'] = forecast_results if 'forecast_results' in locals() else {}\n",
    "globals()['model_performance'] = model_performance if 'model_performance' in locals() else {}\n",
    "globals()['target_column'] = target_column if 'target_column' in locals() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a45d94",
   "metadata": {},
   "source": [
    "## 6. Advanced Time Series Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Interactive Time Series Visualization\n",
    "print(\"ðŸ“Š ADVANCED TIME SERIES VISUALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not ts_df.empty:\n",
    "    \n",
    "    # Step 1: Interactive Time Series Dashboard with Plotly\n",
    "    print(\"ðŸŽ¨ Step 1: Interactive Time Series Dashboard\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Create main time series plot with multiple metrics\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=2,\n",
    "        subplot_titles=['Main Time Series', 'Seasonal Pattern', 'Trend Analysis', 'Forecast Results',\n",
    "                       'Weekly Patterns', 'Monthly Patterns', 'Anomaly Detection', 'Correlation Matrix'],\n",
    "        specs=[[{\"secondary_y\": True}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Main time series with range selector\n",
    "    if target_column and target_column in ts_df.columns:\n",
    "        main_series = ts_df[target_column]\n",
    "        \n",
    "        # Add main time series\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=main_series.index,\n",
    "                y=main_series.values,\n",
    "                name=f'{target_column.replace(\"_\", \" \").title()}',\n",
    "                line=dict(color='blue', width=2),\n",
    "                hovertemplate='Date: %{x}<br>Value: %{y:.2f}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add trend line if available\n",
    "        if 'trend_component' in ts_df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=main_series.index,\n",
    "                    y=ts_df['trend_component'].values,\n",
    "                    name='Trend',\n",
    "                    line=dict(color='red', width=2, dash='dash'),\n",
    "                    hovertemplate='Date: %{x}<br>Trend: %{y:.2f}<extra></extra>'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Add forecasts if available\n",
    "        if 'forecast_results' in globals() and forecast_results:\n",
    "            colors = ['green', 'orange', 'purple', 'brown']\n",
    "            for i, (model_name, result) in enumerate(list(forecast_results.items())[:3]):  # Show top 3 models\n",
    "                forecast = result['forecast']\n",
    "                color = colors[i % len(colors)]\n",
    "                \n",
    "                if hasattr(forecast, 'index') and hasattr(forecast, 'values'):\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=forecast.index,\n",
    "                            y=forecast.values,\n",
    "                            name=f'{model_name} Forecast',\n",
    "                            line=dict(color=color, width=2, dash='dot'),\n",
    "                            hovertemplate=f'{model_name}: %{{y:.2f}}<extra></extra>'\n",
    "                        ),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "        \n",
    "        print(\"âœ… Main time series plot created\")\n",
    "    \n",
    "    # Seasonal pattern visualization\n",
    "    if 'seasonal_component' in ts_df.columns:\n",
    "        seasonal_data = ts_df['seasonal_component'].dropna()\n",
    "        if len(seasonal_data) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=seasonal_data.index,\n",
    "                    y=seasonal_data.values,\n",
    "                    name='Seasonal Component',\n",
    "                    line=dict(color='green', width=1.5),\n",
    "                    fill='tonexty',\n",
    "                    hovertemplate='Date: %{x}<br>Seasonal: %{y:.2f}<extra></extra>'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            print(\"âœ… Seasonal pattern plot created\")\n",
    "    \n",
    "    # Weekly patterns heatmap\n",
    "    if target_column in ts_df.columns:\n",
    "        # Create weekly pattern matrix\n",
    "        weekly_data = ts_df.copy()\n",
    "        weekly_data['week'] = weekly_data.index.isocalendar().week\n",
    "        weekly_data['day_name'] = weekly_data.index.day_name()\n",
    "        \n",
    "        # Aggregate by week and day of week\n",
    "        weekly_pattern = weekly_data.groupby(['week', 'day_name'])[target_column].mean().unstack(fill_value=0)\n",
    "        \n",
    "        if not weekly_pattern.empty:\n",
    "            # Reorder days\n",
    "            day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            weekly_pattern = weekly_pattern.reindex(columns=[d for d in day_order if d in weekly_pattern.columns])\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=weekly_pattern.values,\n",
    "                    x=weekly_pattern.columns,\n",
    "                    y=weekly_pattern.index,\n",
    "                    colorscale='RdYlBu_r',\n",
    "                    name='Weekly Pattern',\n",
    "                    hovertemplate='Week: %{y}<br>Day: %{x}<br>Value: %{z:.2f}<extra></extra>'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            print(\"âœ… Weekly patterns heatmap created\")\n",
    "    \n",
    "    # Monthly patterns\n",
    "    if target_column in ts_df.columns:\n",
    "        monthly_pattern = ts_df.groupby(ts_df.index.month)[target_column].agg(['mean', 'std']).reset_index()\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        monthly_pattern['month_name'] = [month_names[m-1] for m in monthly_pattern['index']]\n",
    "        \n",
    "        # Add monthly averages\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=monthly_pattern['month_name'],\n",
    "                y=monthly_pattern['mean'],\n",
    "                name='Monthly Average',\n",
    "                marker_color='lightblue',\n",
    "                error_y=dict(type='data', array=monthly_pattern['std']),\n",
    "                hovertemplate='Month: %{x}<br>Average: %{y:.2f}<br>Std: %{error_y.array:.2f}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        print(\"âœ… Monthly patterns plot created\")\n",
    "    \n",
    "    # Update layout with range selector\n",
    "    fig.update_layout(\n",
    "        title='Advanced Time Series Analysis Dashboard',\n",
    "        height=1200,\n",
    "        showlegend=True,\n",
    "        xaxis=dict(\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=7, label=\"7D\", step=\"day\", stepmode=\"backward\"),\n",
    "                    dict(count=30, label=\"30D\", step=\"day\", stepmode=\"backward\"),\n",
    "                    dict(count=90, label=\"3M\", step=\"day\", stepmode=\"backward\"),\n",
    "                    dict(count=365, label=\"1Y\", step=\"day\", stepmode=\"backward\"),\n",
    "                    dict(step=\"all\")\n",
    "                ])\n",
    "            ),\n",
    "            rangeslider=dict(visible=True),\n",
    "            type=\"date\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    print(\"âœ… Interactive dashboard created\")\n",
    "    \n",
    "    # Step 2: Anomaly Detection Visualization\n",
    "    print(f\"\\nðŸ” Step 2: Anomaly Detection Visualization\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    if target_column in ts_df.columns:\n",
    "        # Simple anomaly detection using statistical methods\n",
    "        series_data = ts_df[target_column].copy()\n",
    "        \n",
    "        # Calculate rolling statistics\n",
    "        window = min(30, len(series_data) // 4)\n",
    "        rolling_mean = series_data.rolling(window=window, center=True).mean()\n",
    "        rolling_std = series_data.rolling(window=window, center=True).std()\n",
    "        \n",
    "        # Define anomaly thresholds (2 standard deviations)\n",
    "        upper_threshold = rolling_mean + 2 * rolling_std\n",
    "        lower_threshold = rolling_mean - 2 * rolling_std\n",
    "        \n",
    "        # Identify anomalies\n",
    "        anomalies_upper = series_data > upper_threshold\n",
    "        anomalies_lower = series_data < lower_threshold\n",
    "        anomalies = anomalies_upper | anomalies_lower\n",
    "        \n",
    "        # Create anomaly detection plot\n",
    "        fig_anomaly = go.Figure()\n",
    "        \n",
    "        # Add main series\n",
    "        fig_anomaly.add_trace(\n",
    "            go.Scatter(\n",
    "                x=series_data.index,\n",
    "                y=series_data.values,\n",
    "                name='Time Series',\n",
    "                line=dict(color='blue', width=1),\n",
    "                hovertemplate='Date: %{x}<br>Value: %{y:.2f}<extra></extra>'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add confidence bands\n",
    "        fig_anomaly.add_trace(\n",
    "            go.Scatter(\n",
    "                x=upper_threshold.index,\n",
    "                y=upper_threshold.values,\n",
    "                name='Upper Threshold',\n",
    "                line=dict(color='red', width=1, dash='dash'),\n",
    "                fill=None\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig_anomaly.add_trace(\n",
    "            go.Scatter(\n",
    "                x=lower_threshold.index,\n",
    "                y=lower_threshold.values,\n",
    "                name='Lower Threshold',\n",
    "                line=dict(color='red', width=1, dash='dash'),\n",
    "                fill='tonexty',\n",
    "                fillcolor='rgba(255,0,0,0.1)'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Highlight anomalies\n",
    "        if anomalies.any():\n",
    "            anomaly_dates = series_data[anomalies].index\n",
    "            anomaly_values = series_data[anomalies].values\n",
    "            \n",
    "            fig_anomaly.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=anomaly_dates,\n",
    "                    y=anomaly_values,\n",
    "                    mode='markers',\n",
    "                    name='Anomalies',\n",
    "                    marker=dict(color='red', size=8, symbol='x'),\n",
    "                    hovertemplate='Anomaly<br>Date: %{x}<br>Value: %{y:.2f}<extra></extra>'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            print(f\"ðŸš¨ Found {anomalies.sum()} anomalies ({anomalies.sum()/len(series_data)*100:.1f}% of data)\")\n",
    "        else:\n",
    "            print(\"âœ… No significant anomalies detected\")\n",
    "        \n",
    "        fig_anomaly.update_layout(\n",
    "            title='Time Series Anomaly Detection',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Value',\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        fig_anomaly.show()\n",
    "        print(\"âœ… Anomaly detection visualization created\")\n",
    "    \n",
    "    # Step 3: Correlation Analysis Visualization  \n",
    "    print(f\"\\nðŸ”— Step 3: Correlation Analysis Visualization\")\n",
    "    print(\"-\" * 47)\n",
    "    \n",
    "    # Select numerical columns for correlation analysis\n",
    "    numeric_cols = ts_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove highly correlated or redundant features\n",
    "    excluded_patterns = ['lag_', 'rolling_', '_sin', '_cos']\n",
    "    core_features = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if not any(pattern in col for pattern in excluded_patterns):\n",
    "            core_features.append(col)\n",
    "    \n",
    "    # Limit to most relevant features\n",
    "    core_features = core_features[:15]  # Top 15 features\n",
    "    \n",
    "    if len(core_features) > 2:\n",
    "        correlation_matrix = ts_df[core_features].corr()\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        fig_corr = go.Figure(data=go.Heatmap(\n",
    "            z=correlation_matrix.values,\n",
    "            x=correlation_matrix.columns,\n",
    "            y=correlation_matrix.columns,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,\n",
    "            text=correlation_matrix.round(2).values,\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 10},\n",
    "            hovertemplate='%{x} vs %{y}<br>Correlation: %{z:.3f}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig_corr.update_layout(\n",
    "            title='Feature Correlation Matrix',\n",
    "            width=800,\n",
    "            height=800\n",
    "        )\n",
    "        \n",
    "        fig_corr.show()\n",
    "        print(f\"âœ… Correlation matrix created with {len(core_features)} features\")\n",
    "    \n",
    "    # Step 4: Forecast Confidence Intervals\n",
    "    print(f\"\\nðŸ“ˆ Step 4: Forecast Confidence Intervals\")\n",
    "    print(\"-\" * 42)\n",
    "    \n",
    "    if 'forecast_results' in globals() and forecast_results:\n",
    "        # Create detailed forecast plot with confidence intervals\n",
    "        fig_forecast = go.Figure()\n",
    "        \n",
    "        # Add historical data\n",
    "        if target_column in ts_df.columns:\n",
    "            historical_data = ts_df[target_column]\n",
    "            \n",
    "            # Split into training and test\n",
    "            if 'train_size' in locals():\n",
    "                train_data = historical_data.iloc[:train_size]\n",
    "                test_data = historical_data.iloc[train_size:]\n",
    "                \n",
    "                fig_forecast.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=train_data.index,\n",
    "                        y=train_data.values,\n",
    "                        name='Training Data',\n",
    "                        line=dict(color='blue', width=2),\n",
    "                        hovertemplate='Training<br>Date: %{x}<br>Value: %{y:.2f}<extra></extra>'\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                fig_forecast.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=test_data.index,\n",
    "                        y=test_data.values,\n",
    "                        name='Actual Test Data',\n",
    "                        line=dict(color='black', width=3),\n",
    "                        hovertemplate='Actual<br>Date: %{x}<br>Value: %{y:.2f}<extra></extra>'\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # Add best forecast model with confidence intervals\n",
    "        if forecast_results:\n",
    "            best_model_name = list(forecast_results.keys())[0]  # Take first available model\n",
    "            best_forecast = forecast_results[best_model_name]['forecast']\n",
    "            \n",
    "            if hasattr(best_forecast, 'index') and hasattr(best_forecast, 'values'):\n",
    "                # Create simple confidence intervals (Â±10% for demonstration)\n",
    "                forecast_values = best_forecast.values\n",
    "                lower_bound = forecast_values * 0.9\n",
    "                upper_bound = forecast_values * 1.1\n",
    "                \n",
    "                # Add confidence interval\n",
    "                fig_forecast.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=best_forecast.index,\n",
    "                        y=upper_bound,\n",
    "                        fill=None,\n",
    "                        mode='lines',\n",
    "                        line_color='rgba(0,100,80,0)',\n",
    "                        showlegend=False\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                fig_forecast.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=best_forecast.index,\n",
    "                        y=lower_bound,\n",
    "                        fill='tonexty',\n",
    "                        mode='lines',\n",
    "                        line_color='rgba(0,100,80,0)',\n",
    "                        name='Confidence Interval',\n",
    "                        fillcolor='rgba(0,100,80,0.2)'\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Add forecast line\n",
    "                fig_forecast.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=best_forecast.index,\n",
    "                        y=forecast_values,\n",
    "                        name=f'{best_model_name} Forecast',\n",
    "                        line=dict(color='red', width=3, dash='dash'),\n",
    "                        hovertemplate=f'{best_model_name}<br>Date: %{{x}}<br>Forecast: %{{y:.2f}}<extra></extra>'\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        fig_forecast.update_layout(\n",
    "            title='Detailed Forecast with Confidence Intervals',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Value',\n",
    "            height=600,\n",
    "            hovermode='x unified'\n",
    "        )\n",
    "        \n",
    "        fig_forecast.show()\n",
    "        print(\"âœ… Detailed forecast visualization created\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No time series data available for visualization\")\n",
    "\n",
    "print(f\"\\nâœ… ADVANCED TIME SERIES VISUALIZATION COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸŽ¨ Created interactive dashboards with:\")\n",
    "print(\"   â€¢ Multi-dimensional time series plots\")\n",
    "print(\"   â€¢ Range selectors and zoom functionality\")  \n",
    "print(\"   â€¢ Anomaly detection highlights\")\n",
    "print(\"   â€¢ Seasonal pattern heatmaps\")\n",
    "print(\"   â€¢ Forecast confidence intervals\")\n",
    "print(\"   â€¢ Correlation analysis visualizations\")\n",
    "print(\"ðŸš€ Ready for booking demand prediction analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3df74f",
   "metadata": {},
   "source": [
    "## 7. Booking Demand Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5268fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Booking Demand Prediction and Analysis\n",
    "print(\"ðŸŽ¯ BOOKING DEMAND PREDICTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not ts_df.empty:\n",
    "    \n",
    "    # Step 1: Demand Pattern Analysis\n",
    "    print(\"ðŸ“Š Step 1: Demand Pattern Analysis\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Analyze booking patterns by different segments\n",
    "    demand_insights = {}\n",
    "    \n",
    "    if target_column and target_column in ts_df.columns:\n",
    "        main_demand = ts_df[target_column].copy()\n",
    "        \n",
    "        # Overall demand statistics\n",
    "        demand_stats = {\n",
    "            'mean_daily_demand': main_demand.mean(),\n",
    "            'peak_demand': main_demand.max(),\n",
    "            'min_demand': main_demand.min(),\n",
    "            'demand_volatility': main_demand.std() / main_demand.mean(),\n",
    "            'peak_date': main_demand.idxmax(),\n",
    "            'low_date': main_demand.idxmin()\n",
    "        }\n",
    "        \n",
    "        demand_insights['overall'] = demand_stats\n",
    "        \n",
    "        print(f\"ðŸ“ˆ Overall Demand Statistics:\")\n",
    "        print(f\"   Average daily demand: {demand_stats['mean_daily_demand']:.2f}\")\n",
    "        print(f\"   Peak demand: {demand_stats['peak_demand']:.2f} (on {demand_stats['peak_date'].strftime('%Y-%m-%d')})\")\n",
    "        print(f\"   Minimum demand: {demand_stats['min_demand']:.2f} (on {demand_stats['low_date'].strftime('%Y-%m-%d')})\")\n",
    "        print(f\"   Demand volatility: {demand_stats['demand_volatility']:.3f}\")\n",
    "        \n",
    "        # Seasonal demand analysis\n",
    "        seasonal_demand = ts_df.groupby('season')[target_column].agg(['mean', 'std', 'max', 'min']).round(2)\n",
    "        demand_insights['seasonal'] = seasonal_demand.to_dict()\n",
    "        \n",
    "        print(f\"\\nðŸŒ¸ Seasonal Demand Patterns:\")\n",
    "        for season in seasonal_demand.index:\n",
    "            stats = seasonal_demand.loc[season]\n",
    "            print(f\"   {season}: Avg={stats['mean']:.2f}, Peak={stats['max']:.2f}, Low={stats['min']:.2f}\")\n",
    "        \n",
    "        # Weekly demand patterns\n",
    "        weekly_demand = ts_df.groupby('day_of_week')[target_column].mean()\n",
    "        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        \n",
    "        print(f\"\\nðŸ“… Weekly Demand Patterns:\")\n",
    "        for day_idx, avg_demand in weekly_demand.items():\n",
    "            if day_idx < len(day_names):\n",
    "                day_name = day_names[int(day_idx)]\n",
    "                print(f\"   {day_name}: {avg_demand:.2f}\")\n",
    "        \n",
    "        # Monthly demand trends\n",
    "        monthly_demand = ts_df.groupby('month')[target_column].mean()\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        \n",
    "        print(f\"\\nðŸ“† Monthly Demand Trends:\")\n",
    "        for month_idx, avg_demand in monthly_demand.items():\n",
    "            if month_idx <= len(month_names):\n",
    "                month_name = month_names[int(month_idx) - 1]\n",
    "                print(f\"   {month_name}: {avg_demand:.2f}\")\n",
    "    \n",
    "    # Step 2: Demand Forecasting Models\n",
    "    print(f\"\\nðŸ”® Step 2: Advanced Demand Forecasting\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'forecast_results' in globals() and forecast_results:\n",
    "        # Extended forecast horizon for demand planning\n",
    "        extended_horizon = min(90, len(ts_df) // 4)  # 90 days or 25% of data\n",
    "        \n",
    "        print(f\"ðŸŽ¯ Creating extended forecasts for {extended_horizon} days\")\n",
    "        \n",
    "        # Use best performing model for extended forecast\n",
    "        if 'model_performance' in globals() and model_performance:\n",
    "            best_model_name = min(model_performance.keys(), key=lambda x: model_performance[x]['MAE'])\n",
    "            print(f\"ðŸ† Using best model: {best_model_name}\")\n",
    "            \n",
    "            # Create extended forecast scenarios\n",
    "            scenarios = {\n",
    "                'optimistic': 1.15,    # 15% increase\n",
    "                'baseline': 1.0,       # No change\n",
    "                'pessimistic': 0.85    # 15% decrease\n",
    "            }\n",
    "            \n",
    "            extended_forecasts = {}\n",
    "            \n",
    "            if best_model_name in forecast_results:\n",
    "                best_result = forecast_results[best_model_name]\n",
    "                base_forecast = best_result['forecast']\n",
    "                \n",
    "                # Create scenarios\n",
    "                for scenario_name, multiplier in scenarios.items():\n",
    "                    if hasattr(base_forecast, 'values'):\n",
    "                        scenario_forecast = base_forecast.values * multiplier\n",
    "                        extended_forecasts[scenario_name] = pd.Series(\n",
    "                            scenario_forecast, \n",
    "                            index=base_forecast.index\n",
    "                        )\n",
    "                    else:\n",
    "                        scenario_forecast = base_forecast * multiplier\n",
    "                        # Create index for scenario\n",
    "                        last_date = ts_df.index[-1]\n",
    "                        forecast_dates = pd.date_range(\n",
    "                            start=last_date + timedelta(days=1),\n",
    "                            periods=len(scenario_forecast),\n",
    "                            freq='D'\n",
    "                        )\n",
    "                        extended_forecasts[scenario_name] = pd.Series(\n",
    "                            scenario_forecast,\n",
    "                            index=forecast_dates\n",
    "                        )\n",
    "                \n",
    "                print(f\"âœ… Created {len(scenarios)} demand scenarios\")\n",
    "                \n",
    "                # Visualize demand scenarios  \n",
    "                fig_scenarios = go.Figure()\n",
    "                \n",
    "                # Add historical data\n",
    "                if target_column in ts_df.columns:\n",
    "                    recent_data = ts_df[target_column].tail(60)  # Last 60 days\n",
    "                    fig_scenarios.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=recent_data.index,\n",
    "                            y=recent_data.values,\n",
    "                            name='Historical Demand',\n",
    "                            line=dict(color='blue', width=2)\n",
    "                        )\n",
    "                    )\n",
    "                \n",
    "                # Add scenario forecasts\n",
    "                colors = {'optimistic': 'green', 'baseline': 'orange', 'pessimistic': 'red'}\n",
    "                for scenario_name, forecast_data in extended_forecasts.items():\n",
    "                    fig_scenarios.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=forecast_data.index,\n",
    "                            y=forecast_data.values,\n",
    "                            name=f'{scenario_name.title()} Scenario',\n",
    "                            line=dict(color=colors[scenario_name], width=2, dash='dash')\n",
    "                        )\n",
    "                    )\n",
    "                \n",
    "                fig_scenarios.update_layout(\n",
    "                    title='Booking Demand Scenarios',\n",
    "                    xaxis_title='Date',\n",
    "                    yaxis_title='Predicted Demand',\n",
    "                    height=600\n",
    "                )\n",
    "                \n",
    "                fig_scenarios.show()\n",
    "                print(\"âœ… Demand scenario visualization created\")\n",
    "    \n",
    "    # Step 3: Demand Heatmaps and Pattern Recognition\n",
    "    print(f\"\\nðŸ”¥ Step 3: Demand Heatmaps and Patterns\")\n",
    "    print(\"-\" * 42)\n",
    "    \n",
    "    if target_column in ts_df.columns:\n",
    "        # Create demand heatmap by month and day of week\n",
    "        demand_pivot = ts_df.pivot_table(\n",
    "            values=target_column,\n",
    "            index=ts_df.index.month,\n",
    "            columns=ts_df.index.dayofweek,\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Map month and day names\n",
    "        month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        \n",
    "        fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "            z=demand_pivot.values,\n",
    "            x=[day_labels[i] for i in demand_pivot.columns],\n",
    "            y=[month_labels[i-1] for i in demand_pivot.index],\n",
    "            colorscale='RdYlBu_r',\n",
    "            hovertemplate='Month: %{y}<br>Day: %{x}<br>Demand: %{z:.2f}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig_heatmap.update_layout(\n",
    "            title='Booking Demand Heatmap - Month vs Day of Week',\n",
    "            xaxis_title='Day of Week',\n",
    "            yaxis_title='Month',\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        fig_heatmap.show()\n",
    "        print(\"âœ… Demand heatmap created\")\n",
    "        \n",
    "        # Identify peak demand periods\n",
    "        peak_threshold = demand_pivot.quantile(0.8).max()  # Top 20%\n",
    "        peak_periods = []\n",
    "        \n",
    "        for month_idx, month_data in demand_pivot.iterrows():\n",
    "            for day_idx, demand_val in month_data.items():\n",
    "                if pd.notna(demand_val) and demand_val >= peak_threshold:\n",
    "                    peak_periods.append({\n",
    "                        'month': month_labels[month_idx-1],\n",
    "                        'day': day_labels[day_idx],\n",
    "                        'demand': demand_val\n",
    "                    })\n",
    "        \n",
    "        if peak_periods:\n",
    "            print(f\"\\nðŸŽ¯ Peak Demand Periods (Top 20%):\")\n",
    "            for period in sorted(peak_periods, key=lambda x: x['demand'], reverse=True)[:10]:\n",
    "                print(f\"   {period['month']} {period['day']}: {period['demand']:.2f}\")\n",
    "    \n",
    "    # Step 4: Early Warning System for Demand Fluctuations\n",
    "    print(f\"\\nâš ï¸ Step 4: Demand Fluctuation Warning System\")\n",
    "    print(\"-\" * 48)\n",
    "    \n",
    "    if target_column in ts_df.columns:\n",
    "        # Calculate demand volatility indicators\n",
    "        demand_series = ts_df[target_column].copy()\n",
    "        \n",
    "        # Rolling volatility (30-day window)\n",
    "        rolling_volatility = demand_series.rolling(window=30).std()\n",
    "        volatility_threshold = rolling_volatility.quantile(0.8)  # Top 20% volatility\n",
    "        \n",
    "        # Identify high volatility periods\n",
    "        high_volatility_periods = rolling_volatility[rolling_volatility > volatility_threshold]\n",
    "        \n",
    "        print(f\"ðŸ“Š Demand Volatility Analysis:\")\n",
    "        print(f\"   Average volatility: {rolling_volatility.mean():.2f}\")\n",
    "        print(f\"   High volatility threshold: {volatility_threshold:.2f}\")\n",
    "        print(f\"   High volatility periods: {len(high_volatility_periods)}\")\n",
    "        \n",
    "        # Early warning indicators\n",
    "        recent_volatility = rolling_volatility.tail(7).mean()  # Last week average\n",
    "        volatility_trend = 'Increasing' if recent_volatility > rolling_volatility.mean() else 'Stable'\n",
    "        \n",
    "        # Demand trend analysis\n",
    "        recent_demand = demand_series.tail(7).mean()\n",
    "        historical_demand = demand_series.mean()\n",
    "        demand_change = (recent_demand - historical_demand) / historical_demand * 100\n",
    "        \n",
    "        print(f\"\\nðŸš¨ Current Demand Alert Status:\")\n",
    "        print(f\"   Recent demand trend: {demand_change:+.1f}% vs historical average\")\n",
    "        print(f\"   Volatility trend: {volatility_trend}\")\n",
    "        \n",
    "        # Generate alerts\n",
    "        alerts = []\n",
    "        if abs(demand_change) > 20:\n",
    "            alert_type = \"MAJOR\" if abs(demand_change) > 30 else \"MODERATE\"\n",
    "            direction = \"increase\" if demand_change > 0 else \"decrease\"\n",
    "            alerts.append(f\"{alert_type} demand {direction} detected ({demand_change:+.1f}%)\")\n",
    "        \n",
    "        if recent_volatility > volatility_threshold:\n",
    "            alerts.append(\"HIGH volatility detected - demand unpredictability increased\")\n",
    "        \n",
    "        if alerts:\n",
    "            print(f\"\\nðŸš¨ ACTIVE ALERTS:\")\n",
    "            for alert in alerts:\n",
    "                print(f\"   â€¢ {alert}\")\n",
    "        else:\n",
    "            print(f\"\\nâœ… No significant demand alerts at this time\")\n",
    "    \n",
    "    # Step 5: Scenario Analysis and Business Impact\n",
    "    print(f\"\\nðŸ’¼ Step 5: Business Impact Scenario Analysis\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    if 'extended_forecasts' in locals() and extended_forecasts:\n",
    "        # Calculate business impact metrics for each scenario\n",
    "        scenario_analysis = {}\n",
    "        \n",
    "        for scenario_name, forecast_data in extended_forecasts.items():\n",
    "            total_predicted_demand = forecast_data.sum()\n",
    "            avg_daily_demand = forecast_data.mean()\n",
    "            peak_demand = forecast_data.max()\n",
    "            \n",
    "            # Estimate capacity utilization (assuming max capacity)\n",
    "            estimated_capacity = demand_stats['peak_demand'] * 1.2  # 20% buffer above historical peak\n",
    "            utilization_rate = avg_daily_demand / estimated_capacity\n",
    "            \n",
    "            # Revenue impact (assuming average price per booking)\n",
    "            avg_price_per_booking = 75  # Placeholder - could use actual price data\n",
    "            estimated_revenue = total_predicted_demand * avg_price_per_booking\n",
    "            \n",
    "            scenario_analysis[scenario_name] = {\n",
    "                'total_demand': total_predicted_demand,\n",
    "                'avg_daily_demand': avg_daily_demand,\n",
    "                'peak_demand': peak_demand,\n",
    "                'utilization_rate': utilization_rate,\n",
    "                'estimated_revenue': estimated_revenue\n",
    "            }\n",
    "        \n",
    "        print(f\"ðŸ’° Business Impact Analysis ({extended_horizon} days):\")\n",
    "        for scenario, metrics in scenario_analysis.items():\n",
    "            print(f\"\\n   {scenario.title()} Scenario:\")\n",
    "            print(f\"     Total demand: {metrics['total_demand']:.0f} bookings\")\n",
    "            print(f\"     Daily average: {metrics['avg_daily_demand']:.1f} bookings/day\")\n",
    "            print(f\"     Peak demand: {metrics['peak_demand']:.1f} bookings\")\n",
    "            print(f\"     Capacity utilization: {metrics['utilization_rate']:.1%}\")\n",
    "            print(f\"     Estimated revenue: ${metrics['estimated_revenue']:,.0f}\")\n",
    "        \n",
    "        # Business recommendations\n",
    "        baseline_revenue = scenario_analysis['baseline']['estimated_revenue']\n",
    "        optimistic_upside = scenario_analysis['optimistic']['estimated_revenue'] - baseline_revenue\n",
    "        pessimistic_downside = baseline_revenue - scenario_analysis['pessimistic']['estimated_revenue']\n",
    "        \n",
    "        print(f\"\\nðŸ’¡ Business Recommendations:\")\n",
    "        print(f\"   Revenue opportunity: ${optimistic_upside:,.0f} upside potential\")\n",
    "        print(f\"   Revenue risk: ${pessimistic_downside:,.0f} downside exposure\")\n",
    "        print(f\"   Recommended strategy: Diversify booking channels and optimize pricing\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No time series data available for demand prediction\")\n",
    "\n",
    "print(f\"\\nâœ… BOOKING DEMAND PREDICTION COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸŽ¯ Delivered comprehensive demand intelligence:\")\n",
    "print(\"   â€¢ Seasonal and weekly demand patterns\")\n",
    "print(\"   â€¢ Multi-scenario demand forecasts\")\n",
    "print(\"   â€¢ Early warning system for fluctuations\")\n",
    "print(\"   â€¢ Business impact analysis with revenue projections\")\n",
    "print(\"ðŸš€ Ready for seasonal pattern deep-dive analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca81cd",
   "metadata": {},
   "source": [
    "## 8. Seasonal Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Dive Seasonal Pattern Analysis\n",
    "print(\"ðŸŒŠ SEASONAL PATTERN DEEP DIVE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not ts_df.empty:\n",
    "    \n",
    "    # Step 1: Multi-level Seasonal Analysis\n",
    "    print(\"ðŸ”¬ Step 1: Multi-level Seasonal Decomposition\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    if target_column and target_column in ts_df.columns:\n",
    "        main_series = ts_df[target_column].copy()\n",
    "        \n",
    "        # Analyze different seasonal patterns\n",
    "        seasonal_patterns = {}\n",
    "        \n",
    "        # Daily patterns (hourly would be ideal, but we'll simulate business hours effect)\n",
    "        ts_df['business_hour_effect'] = np.sin(2 * np.pi * ts_df['day_of_year'] / 365.25 + np.pi/6) * 0.15\n",
    "        \n",
    "        # Weekly patterns\n",
    "        weekly_pattern = ts_df.groupby('day_of_week')[target_column].mean()\n",
    "        seasonal_patterns['weekly'] = weekly_pattern\n",
    "        \n",
    "        # Monthly patterns  \n",
    "        monthly_pattern = ts_df.groupby('month')[target_column].mean()\n",
    "        seasonal_patterns['monthly'] = monthly_pattern\n",
    "        \n",
    "        # Quarterly patterns\n",
    "        quarterly_pattern = ts_df.groupby('quarter')[target_column].mean()\n",
    "        seasonal_patterns['quarterly'] = quarterly_pattern\n",
    "        \n",
    "        # Annual patterns (if we have multiple years)\n",
    "        if ts_df['year'].nunique() > 1:\n",
    "            annual_pattern = ts_df.groupby('year')[target_column].mean()\n",
    "            seasonal_patterns['annual'] = annual_pattern\n",
    "        \n",
    "        print(f\"âœ… Identified {len(seasonal_patterns)} seasonal patterns\")\n",
    "        \n",
    "        # Calculate seasonal strength for each pattern\n",
    "        print(f\"\\nðŸ“Š Seasonal Strength Analysis:\")\n",
    "        for pattern_name, pattern_data in seasonal_patterns.items():\n",
    "            if len(pattern_data) > 1:\n",
    "                seasonal_variance = pattern_data.var()\n",
    "                total_variance = main_series.var()\n",
    "                seasonal_strength = seasonal_variance / total_variance\n",
    "                \n",
    "                print(f\"   {pattern_name.title()}: {seasonal_strength:.3f} ({('Strong' if seasonal_strength > 0.1 else 'Moderate' if seasonal_strength > 0.05 else 'Weak')})\")\n",
    "    \n",
    "    # Step 2: Peak/Off-Peak Period Identification\n",
    "    print(f\"\\nðŸŽ¯ Step 2: Peak/Off-Peak Period Analysis\")\n",
    "    print(\"-\" * 42)\n",
    "    \n",
    "    if target_column in ts_df.columns:\n",
    "        # Define peak periods based on percentiles\n",
    "        series_data = ts_df[target_column]\n",
    "        peak_threshold = series_data.quantile(0.75)  # Top 25%\n",
    "        off_peak_threshold = series_data.quantile(0.25)  # Bottom 25%\n",
    "        \n",
    "        # Classify periods\n",
    "        ts_df['demand_level'] = 'Normal'\n",
    "        ts_df.loc[series_data >= peak_threshold, 'demand_level'] = 'Peak'\n",
    "        ts_df.loc[series_data <= off_peak_threshold, 'demand_level'] = 'Off-Peak'\n",
    "        \n",
    "        # Analyze peak periods by different dimensions\n",
    "        peak_analysis = {}\n",
    "        \n",
    "        # By season\n",
    "        season_peaks = ts_df[ts_df['demand_level'] == 'Peak'].groupby('season').size()\n",
    "        season_total = ts_df.groupby('season').size()\n",
    "        season_peak_rate = (season_peaks / season_total * 100).fillna(0)\n",
    "        \n",
    "        peak_analysis['seasonal'] = season_peak_rate.to_dict()\n",
    "        \n",
    "        print(f\"ðŸ”ï¸ Peak Period Distribution by Season:\")\n",
    "        for season, rate in season_peak_rate.items():\n",
    "            print(f\"   {season}: {rate:.1f}% of days are peak demand\")\n",
    "        \n",
    "        # By day of week\n",
    "        dow_peaks = ts_df[ts_df['demand_level'] == 'Peak'].groupby('day_of_week').size()\n",
    "        dow_total = ts_df.groupby('day_of_week').size()\n",
    "        dow_peak_rate = (dow_peaks / dow_total * 100).fillna(0)\n",
    "        \n",
    "        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        print(f\"\\nðŸ“… Peak Period Distribution by Day of Week:\")\n",
    "        for day_idx, rate in dow_peak_rate.items():\n",
    "            if day_idx < len(day_names):\n",
    "                print(f\"   {day_names[int(day_idx)]}: {rate:.1f}% of days are peak demand\")\n",
    "        \n",
    "        # By month\n",
    "        month_peaks = ts_df[ts_df['demand_level'] == 'Peak'].groupby('month').size()\n",
    "        month_total = ts_df.groupby('month').size()\n",
    "        month_peak_rate = (month_peaks / month_total * 100).fillna(0)\n",
    "        \n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        print(f\"\\nðŸ“† Peak Period Distribution by Month:\")\n",
    "        for month_idx, rate in month_peak_rate.items():\n",
    "            if month_idx <= len(month_names):\n",
    "                print(f\"   {month_names[int(month_idx)-1]}: {rate:.1f}% of days are peak demand\")\n",
    "    \n",
    "    # Step 3: Holiday Effects Analysis\n",
    "    print(f\"\\nðŸŽ‰ Step 3: Holiday Effects Analysis\")\n",
    "    print(\"-\" * 38)\n",
    "    \n",
    "    # Analyze demand around holidays (using our created holiday features)\n",
    "    holiday_effects = {}\n",
    "    \n",
    "    holiday_columns = [col for col in ts_df.columns if col.startswith('is_') and col != 'is_weekend']\n",
    "    \n",
    "    if holiday_columns and target_column in ts_df.columns:\n",
    "        print(f\"ðŸŽŠ Analyzing {len(holiday_columns)} holiday effects:\")\n",
    "        \n",
    "        for holiday_col in holiday_columns:\n",
    "            holiday_name = holiday_col.replace('is_', '').replace('_', ' ').title()\n",
    "            \n",
    "            # Compare demand on holiday vs non-holiday days\n",
    "            holiday_demand = ts_df[ts_df[holiday_col] == 1][target_column].mean()\n",
    "            normal_demand = ts_df[ts_df[holiday_col] == 0][target_column].mean()\n",
    "            \n",
    "            if pd.notna(holiday_demand) and pd.notna(normal_demand):\n",
    "                effect_percentage = ((holiday_demand - normal_demand) / normal_demand) * 100\n",
    "                holiday_effects[holiday_name] = {\n",
    "                    'holiday_demand': holiday_demand,\n",
    "                    'normal_demand': normal_demand,\n",
    "                    'effect_percentage': effect_percentage\n",
    "                }\n",
    "                \n",
    "                effect_direction = \"increase\" if effect_percentage > 0 else \"decrease\"\n",
    "                print(f\"   {holiday_name}: {effect_percentage:+.1f}% {effect_direction}\")\n",
    "    \n",
    "    # Step 4: Weather Correlation (Simulated)\n",
    "    print(f\"\\nðŸŒ¤ï¸ Step 4: Weather Impact Analysis (Simulated)\")\n",
    "    print(\"-\" * 48)\n",
    "    \n",
    "    # Since we don't have actual weather data, we'll simulate weather effects\n",
    "    # based on seasonal patterns typical for Bali\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate weather variables\n",
    "    ts_df['temperature'] = 28 + 3 * np.sin(2 * np.pi * ts_df['day_of_year'] / 365.25) + np.random.normal(0, 2, len(ts_df))\n",
    "    ts_df['rainfall_prob'] = 0.3 + 0.2 * np.sin(2 * np.pi * (ts_df['day_of_year'] - 120) / 365.25)  # Rainy season offset\n",
    "    ts_df['rainfall_prob'] = np.clip(ts_df['rainfall_prob'], 0, 1)\n",
    "    \n",
    "    # Analyze correlation with demand\n",
    "    if target_column in ts_df.columns:\n",
    "        temp_corr = ts_df[target_column].corr(ts_df['temperature'])\n",
    "        rain_corr = ts_df[target_column].corr(ts_df['rainfall_prob'])\n",
    "        \n",
    "        print(f\"ðŸŒ¡ï¸ Weather Correlation Analysis:\")\n",
    "        print(f\"   Temperature correlation: {temp_corr:.3f}\")\n",
    "        print(f\"   Rainfall probability correlation: {rain_corr:.3f}\")\n",
    "        \n",
    "        # Create weather impact visualization\n",
    "        fig_weather = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=['Temperature vs Demand', 'Rainfall Probability vs Demand'],\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Temperature scatter\n",
    "        fig_weather.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ts_df['temperature'],\n",
    "                y=ts_df[target_column],\n",
    "                mode='markers',\n",
    "                name='Temperature vs Demand',\n",
    "                marker=dict(color='red', alpha=0.6),\n",
    "                hovertemplate='Temperature: %{x:.1f}Â°C<br>Demand: %{y:.2f}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Rainfall scatter\n",
    "        fig_weather.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ts_df['rainfall_prob'],\n",
    "                y=ts_df[target_column],\n",
    "                mode='markers',\n",
    "                name='Rainfall vs Demand',\n",
    "                marker=dict(color='blue', alpha=0.6),\n",
    "                hovertemplate='Rainfall Prob: %{x:.2f}<br>Demand: %{y:.2f}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig_weather.update_layout(\n",
    "            title='Weather Impact on Booking Demand',\n",
    "            height=800\n",
    "        )\n",
    "        \n",
    "        fig_weather.show()\n",
    "        print(\"âœ… Weather impact visualization created\")\n",
    "    \n",
    "    # Step 5: Seasonal Adjustment Factors\n",
    "    print(f\"\\nâš–ï¸ Step 5: Seasonal Adjustment Factors\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if target_column in ts_df.columns and 'seasonal_component' in ts_df.columns:\n",
    "        # Calculate seasonal adjustment factors for business planning\n",
    "        seasonal_factors = {}\n",
    "        \n",
    "        # Monthly seasonal factors\n",
    "        monthly_seasonal = ts_df.groupby('month')['seasonal_component'].mean()\n",
    "        monthly_factors = (monthly_seasonal / monthly_seasonal.mean())\n",
    "        seasonal_factors['monthly'] = monthly_factors.to_dict()\n",
    "        \n",
    "        print(f\"ðŸ“Š Monthly Seasonal Adjustment Factors:\")\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        for month_idx, factor in monthly_factors.items():\n",
    "            month_name = month_names[int(month_idx)-1]\n",
    "            percentage = (factor - 1) * 100\n",
    "            print(f\"   {month_name}: {factor:.3f} ({percentage:+.1f}%)\")\n",
    "        \n",
    "        # Weekly seasonal factors\n",
    "        weekly_seasonal = ts_df.groupby('day_of_week')['seasonal_component'].mean()\n",
    "        weekly_factors = (weekly_seasonal / weekly_seasonal.mean())\n",
    "        seasonal_factors['weekly'] = weekly_factors.to_dict()\n",
    "        \n",
    "        print(f\"\\nðŸ“… Weekly Seasonal Adjustment Factors:\")\n",
    "        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        for day_idx, factor in weekly_factors.items():\n",
    "            if day_idx < len(day_names):\n",
    "                day_name = day_names[int(day_idx)]\n",
    "                percentage = (factor - 1) * 100\n",
    "                print(f\"   {day_name}: {factor:.3f} ({percentage:+.1f}%)\")\n",
    "        \n",
    "        # Create seasonal adjustment visualization\n",
    "        fig_seasonal = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=['Monthly Factors', 'Weekly Factors']\n",
    "        )\n",
    "        \n",
    "        # Monthly factors\n",
    "        fig_seasonal.add_trace(\n",
    "            go.Bar(\n",
    "                x=[month_names[int(i)-1] for i in monthly_factors.index],\n",
    "                y=monthly_factors.values,\n",
    "                name='Monthly Factors',\n",
    "                marker_color='lightblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Weekly factors\n",
    "        fig_seasonal.add_trace(\n",
    "            go.Bar(\n",
    "                x=[day_names[int(i)] for i in weekly_factors.index if int(i) < len(day_names)],\n",
    "                y=[weekly_factors[i] for i in weekly_factors.index if int(i) < len(day_names)],\n",
    "                name='Weekly Factors',\n",
    "                marker_color='lightgreen'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Add reference line at 1.0\n",
    "        for col in [1, 2]:\n",
    "            fig_seasonal.add_hline(y=1.0, line_dash=\"dash\", line_color=\"red\", row=1, col=col)\n",
    "        \n",
    "        fig_seasonal.update_layout(\n",
    "            title='Seasonal Adjustment Factors for Business Planning',\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        fig_seasonal.show()\n",
    "        print(\"âœ… Seasonal adjustment factors visualization created\")\n",
    "    \n",
    "    # Step 6: Business Planning Recommendations\n",
    "    print(f\"\\nðŸ’¼ Step 6: Seasonal Business Planning\")\n",
    "    print(\"-\" * 38)\n",
    "    \n",
    "    if 'seasonal_factors' in locals():\n",
    "        print(f\"ðŸŽ¯ Strategic Seasonal Recommendations:\")\n",
    "        \n",
    "        # Identify high and low seasons\n",
    "        if 'monthly' in seasonal_factors:\n",
    "            monthly_factors_df = pd.Series(seasonal_factors['monthly'])\n",
    "            peak_months = monthly_factors_df.nlargest(3)\n",
    "            low_months = monthly_factors_df.nsmallest(3)\n",
    "            \n",
    "            print(f\"\\nðŸ”ï¸ Peak Season Strategy (Top 3 months):\")\n",
    "            for month_idx, factor in peak_months.items():\n",
    "                month_name = month_names[int(month_idx)-1]\n",
    "                premium = (factor - 1) * 100\n",
    "                print(f\"   {month_name}: Apply {premium:+.1f}% premium pricing\")\n",
    "            \n",
    "            print(f\"\\nðŸ“‰ Off-Season Strategy (Bottom 3 months):\")\n",
    "            for month_idx, factor in low_months.items():\n",
    "                month_name = month_names[int(month_idx)-1]\n",
    "                discount = (1 - factor) * 100\n",
    "                print(f\"   {month_name}: Consider {discount:.1f}% discount or promotional campaigns\")\n",
    "        \n",
    "        # Weekly recommendations\n",
    "        if 'weekly' in seasonal_factors:\n",
    "            weekly_factors_df = pd.Series(seasonal_factors['weekly'])\n",
    "            peak_days = weekly_factors_df.nlargest(2)\n",
    "            low_days = weekly_factors_df.nsmallest(2)\n",
    "            \n",
    "            print(f\"\\nðŸ“… Weekly Optimization:\")\n",
    "            for day_idx, factor in peak_days.items():\n",
    "                if day_idx < len(day_names):\n",
    "                    day_name = day_names[int(day_idx)]\n",
    "                    premium = (factor - 1) * 100\n",
    "                    print(f\"   {day_name}: Peak day - maximize revenue ({premium:+.1f}%)\")\n",
    "            \n",
    "            for day_idx, factor in low_days.items():\n",
    "                if day_idx < len(day_names):\n",
    "                    day_name = day_names[int(day_idx)]\n",
    "                    print(f\"   {day_name}: Focus on increasing occupancy with promotions\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No time series data available for seasonal analysis\")\n",
    "\n",
    "print(f\"\\nâœ… SEASONAL PATTERN ANALYSIS COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸŒŠ Deep seasonal intelligence delivered:\")\n",
    "print(\"   â€¢ Multi-level seasonal decomposition\")\n",
    "print(\"   â€¢ Peak/off-peak period identification\")\n",
    "print(\"   â€¢ Holiday effects quantification\")\n",
    "print(\"   â€¢ Weather impact correlation analysis\")\n",
    "print(\"   â€¢ Seasonal adjustment factors for planning\")\n",
    "print(\"ðŸš€ Ready for revenue optimization analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3d76b",
   "metadata": {},
   "source": [
    "## 9. Revenue Optimization Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Revenue Optimization Analysis\n",
    "print(\"ðŸ’° REVENUE OPTIMIZATION OVER TIME\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not ts_df.empty:\n",
    "    \n",
    "    # Step 1: Revenue Trend Analysis\n",
    "    print(\"ðŸ“ˆ Step 1: Revenue Trend Analysis\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Calculate revenue metrics over time\n",
    "    try:\n",
    "        # Using price as a proxy for revenue (would need booking volume for actual revenue)\n",
    "        if target_column and target_column in ts_df.columns:\n",
    "            revenue_proxy = ts_df[target_column].copy()\n",
    "            \n",
    "            # Create rolling revenue metrics\n",
    "            ts_df['revenue_7d_avg'] = revenue_proxy.rolling(window=7, center=True).mean()\n",
    "            ts_df['revenue_30d_avg'] = revenue_proxy.rolling(window=30, center=True).mean()\n",
    "            ts_df['revenue_90d_avg'] = revenue_proxy.rolling(window=90, center=True).mean()\n",
    "            \n",
    "            # Calculate revenue growth rates\n",
    "            ts_df['revenue_mom_growth'] = revenue_proxy.pct_change(periods=30) * 100  # Month-over-month\n",
    "            ts_df['revenue_yoy_growth'] = revenue_proxy.pct_change(periods=365) * 100  # Year-over-year (if available)\n",
    "            \n",
    "            # Calculate volatility (30-day rolling standard deviation)\n",
    "            ts_df['revenue_volatility'] = revenue_proxy.rolling(window=30).std()\n",
    "            \n",
    "            print(f\"âœ… Revenue trend metrics calculated\")\n",
    "            print(f\"   ðŸ“Š Current average revenue: ${revenue_proxy.mean():.2f}\")\n",
    "            print(f\"   ðŸ“ˆ Revenue volatility (30d): ${ts_df['revenue_volatility'].mean():.2f}\")\n",
    "            \n",
    "            # Identify trend periods\n",
    "            recent_growth = ts_df['revenue_mom_growth'].tail(30).mean()\n",
    "            if pd.notna(recent_growth):\n",
    "                trend_direction = \"increasing\" if recent_growth > 2 else \"decreasing\" if recent_growth < -2 else \"stable\"\n",
    "                print(f\"   ðŸŽ¯ Recent trend (30d): {trend_direction} ({recent_growth:+.1f}%)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in revenue trend analysis: {e}\")\n",
    "    \n",
    "    # Step 2: Optimal Pricing Windows Analysis\n",
    "    print(f\"\\nðŸŽ¯ Step 2: Optimal Pricing Windows\")\n",
    "    print(\"-\" * 36)\n",
    "    \n",
    "    try:\n",
    "        if target_column in ts_df.columns:\n",
    "            # Identify high-revenue periods and their characteristics\n",
    "            revenue_data = ts_df[target_column]\n",
    "            high_revenue_threshold = revenue_data.quantile(0.80)  # Top 20%\n",
    "            low_revenue_threshold = revenue_data.quantile(0.20)   # Bottom 20%\n",
    "            \n",
    "            # Analyze high-revenue periods\n",
    "            high_revenue_periods = ts_df[revenue_data >= high_revenue_threshold].copy()\n",
    "            low_revenue_periods = ts_df[revenue_data <= low_revenue_threshold].copy()\n",
    "            \n",
    "            print(f\"ðŸ† High-Revenue Period Analysis (Top 20%):\")\n",
    "            if not high_revenue_periods.empty:\n",
    "                # Season analysis\n",
    "                high_season_dist = high_revenue_periods['season'].value_counts(normalize=True) * 100\n",
    "                print(f\"   ðŸŒ± Season distribution:\")\n",
    "                for season, pct in high_season_dist.items():\n",
    "                    print(f\"      {season}: {pct:.1f}%\")\n",
    "                \n",
    "                # Day of week analysis\n",
    "                high_dow_dist = high_revenue_periods['day_of_week'].value_counts(normalize=True) * 100\n",
    "                day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "                print(f\"   ðŸ“… Day of week distribution:\")\n",
    "                for dow, pct in high_dow_dist.items():\n",
    "                    if dow < len(day_names):\n",
    "                        print(f\"      {day_names[int(dow)]}: {pct:.1f}%\")\n",
    "                \n",
    "                # Month analysis\n",
    "                high_month_dist = high_revenue_periods['month'].value_counts(normalize=True) * 100\n",
    "                month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "                print(f\"   ðŸ“† Top 3 months for high revenue:\")\n",
    "                for month, pct in high_month_dist.head(3).items():\n",
    "                    print(f\"      {month_names[int(month)-1]}: {pct:.1f}%\")\n",
    "            \n",
    "            print(f\"\\nðŸ“‰ Low-Revenue Period Analysis (Bottom 20%):\")\n",
    "            if not low_revenue_periods.empty:\n",
    "                # Identify improvement opportunities\n",
    "                low_season_dist = low_revenue_periods['season'].value_counts(normalize=True) * 100\n",
    "                print(f\"   ðŸŒ± Seasons needing attention:\")\n",
    "                for season, pct in low_season_dist.head(2).items():\n",
    "                    print(f\"      {season}: {pct:.1f}% of low-revenue periods\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in pricing windows analysis: {e}\")\n",
    "    \n",
    "    # Step 3: Dynamic Pricing Recommendations\n",
    "    print(f\"\\nâš¡ Step 3: Dynamic Pricing Strategy\")\n",
    "    print(\"-\" * 37)\n",
    "    \n",
    "    try:\n",
    "        if 'seasonal_factors' in locals() and target_column in ts_df.columns:\n",
    "            current_price = ts_df[target_column].iloc[-1] if not ts_df.empty else 100\n",
    "            \n",
    "            print(f\"ðŸ’¡ Dynamic Pricing Recommendations:\")\n",
    "            print(f\"   ðŸ“ Current base price: ${current_price:.2f}\")\n",
    "            \n",
    "            # Create pricing calendar for next 365 days\n",
    "            future_dates = pd.date_range(start=ts_df.index[-1] + pd.Timedelta(days=1), \n",
    "                                       periods=365, freq='D')\n",
    "            pricing_calendar = pd.DataFrame(index=future_dates)\n",
    "            pricing_calendar['month'] = pricing_calendar.index.month\n",
    "            pricing_calendar['day_of_week'] = pricing_calendar.index.dayofweek\n",
    "            pricing_calendar['season'] = pricing_calendar.index.month.map({\n",
    "                12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "                3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "                6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "                9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
    "            })\n",
    "            \n",
    "            # Apply seasonal factors if available\n",
    "            if 'monthly' in seasonal_factors:\n",
    "                monthly_factors_dict = seasonal_factors['monthly']\n",
    "                pricing_calendar['monthly_factor'] = pricing_calendar['month'].map(monthly_factors_dict)\n",
    "                pricing_calendar['monthly_factor'] = pricing_calendar['monthly_factor'].fillna(1.0)\n",
    "            else:\n",
    "                pricing_calendar['monthly_factor'] = 1.0\n",
    "            \n",
    "            if 'weekly' in seasonal_factors:\n",
    "                weekly_factors_dict = seasonal_factors['weekly']\n",
    "                pricing_calendar['weekly_factor'] = pricing_calendar['day_of_week'].map(weekly_factors_dict)\n",
    "                pricing_calendar['weekly_factor'] = pricing_calendar['weekly_factor'].fillna(1.0)\n",
    "            else:\n",
    "                pricing_calendar['weekly_factor'] = 1.0\n",
    "            \n",
    "            # Calculate recommended prices\n",
    "            pricing_calendar['base_price'] = current_price\n",
    "            pricing_calendar['seasonal_price'] = (pricing_calendar['base_price'] * \n",
    "                                                pricing_calendar['monthly_factor'] * \n",
    "                                                pricing_calendar['weekly_factor'])\n",
    "            \n",
    "            # Add demand-based adjustments (simplified)\n",
    "            pricing_calendar['demand_adjustment'] = 1.0\n",
    "            pricing_calendar.loc[pricing_calendar['day_of_week'].isin([4, 5, 6]), 'demand_adjustment'] = 1.1  # Weekend premium\n",
    "            \n",
    "            pricing_calendar['recommended_price'] = (pricing_calendar['seasonal_price'] * \n",
    "                                                   pricing_calendar['demand_adjustment'])\n",
    "            \n",
    "            # Analyze pricing opportunities\n",
    "            price_range = pricing_calendar['recommended_price']\n",
    "            avg_recommended = price_range.mean()\n",
    "            max_recommended = price_range.max()\n",
    "            min_recommended = price_range.min()\n",
    "            \n",
    "            print(f\"   ðŸ“Š Annual pricing range: ${min_recommended:.2f} - ${max_recommended:.2f}\")\n",
    "            print(f\"   ðŸ“ˆ Average recommended: ${avg_recommended:.2f}\")\n",
    "            print(f\"   ðŸŽ¯ Potential revenue increase: {((avg_recommended - current_price) / current_price * 100):+.1f}%\")\n",
    "            \n",
    "            # Identify best pricing periods\n",
    "            top_pricing_periods = pricing_calendar.nlargest(10, 'recommended_price')\n",
    "            print(f\"\\nðŸ† Top 10 Premium Pricing Opportunities:\")\n",
    "            for idx, (date, row) in enumerate(top_pricing_periods.iterrows(), 1):\n",
    "                price_premium = ((row['recommended_price'] - current_price) / current_price) * 100\n",
    "                print(f\"   {idx}. {date.strftime('%Y-%m-%d')} ({date.strftime('%A')}): ${row['recommended_price']:.2f} (+{price_premium:.1f}%)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in dynamic pricing analysis: {e}\")\n",
    "    \n",
    "    # Step 4: Revenue Forecasting with Sensitivity Analysis\n",
    "    print(f\"\\nðŸ”® Step 4: Revenue Forecasting & Sensitivity\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    try:\n",
    "        if target_column in ts_df.columns and len(ts_df) > 30:\n",
    "            # Simple revenue forecasting using trend analysis\n",
    "            recent_trend = ts_df[target_column].tail(30).mean()\n",
    "            historical_std = ts_df[target_column].std()\n",
    "            \n",
    "            # Create different scenarios\n",
    "            scenarios = {\n",
    "                'Conservative': recent_trend * 0.95,\n",
    "                'Base Case': recent_trend,\n",
    "                'Optimistic': recent_trend * 1.05,\n",
    "                'Peak Season': recent_trend * 1.15\n",
    "            }\n",
    "            \n",
    "            print(f\"ðŸ“Š Revenue Forecast Scenarios (30-day average):\")\n",
    "            for scenario, value in scenarios.items():\n",
    "                change_pct = ((value - recent_trend) / recent_trend) * 100\n",
    "                print(f\"   {scenario}: ${value:.2f} per night ({change_pct:+.1f}%)\")\n",
    "            \n",
    "            # Sensitivity analysis - impact of pricing changes\n",
    "            print(f\"\\nðŸŽšï¸ Pricing Sensitivity Analysis:\")\n",
    "            price_changes = [-20, -10, -5, 0, 5, 10, 15, 20]\n",
    "            \n",
    "            for price_change in price_changes:\n",
    "                adjusted_price = recent_trend * (1 + price_change/100)\n",
    "                # Assume elasticity of demand (simplified model)\n",
    "                elasticity = -1.2  # Typical for hospitality\n",
    "                demand_change = elasticity * price_change\n",
    "                occupancy_factor = max(0.3, 1 + demand_change/100)  # Minimum 30% occupancy\n",
    "                \n",
    "                revenue_impact = adjusted_price * occupancy_factor\n",
    "                revenue_change = ((revenue_impact - recent_trend) / recent_trend) * 100\n",
    "                \n",
    "                print(f\"   Price {price_change:+d}%: Revenue {revenue_change:+.1f}% (Occupancy factor: {occupancy_factor:.2f})\")\n",
    "            \n",
    "            # Risk assessment\n",
    "            revenue_volatility = ts_df[target_column].rolling(window=30).std().mean()\n",
    "            volatility_pct = (revenue_volatility / recent_trend) * 100\n",
    "            \n",
    "            print(f\"\\nâš ï¸ Risk Assessment:\")\n",
    "            print(f\"   ðŸ’¹ Revenue volatility: Â±{volatility_pct:.1f}%\")\n",
    "            if volatility_pct > 20:\n",
    "                print(f\"   ðŸš¨ High volatility - consider flexible pricing strategies\")\n",
    "            elif volatility_pct < 10:\n",
    "                print(f\"   âœ… Low volatility - stable pricing environment\")\n",
    "            else:\n",
    "                print(f\"   ðŸ“Š Moderate volatility - balanced approach recommended\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in revenue forecasting: {e}\")\n",
    "    \n",
    "    # Step 5: Revenue Optimization Visualization\n",
    "    print(f\"\\nðŸ“Š Step 5: Revenue Optimization Dashboard\")\n",
    "    print(\"-\" * 43)\n",
    "    \n",
    "    try:\n",
    "        if target_column in ts_df.columns:\n",
    "            # Create comprehensive revenue optimization dashboard\n",
    "            fig_revenue = make_subplots(\n",
    "                rows=3, cols=2,\n",
    "                subplot_titles=[\n",
    "                    'Revenue Trends Over Time',\n",
    "                    'Revenue by Season & Day of Week',\n",
    "                    'Monthly Revenue Patterns',\n",
    "                    'Revenue Volatility Analysis',\n",
    "                    'Pricing Opportunity Calendar',\n",
    "                    'Revenue Growth Analysis'\n",
    "                ],\n",
    "                specs=[[{}, {}],\n",
    "                       [{}, {}],\n",
    "                       [{\"colspan\": 2}, None]],\n",
    "                vertical_spacing=0.08\n",
    "            )\n",
    "            \n",
    "            # 1. Revenue trends\n",
    "            if 'revenue_7d_avg' in ts_df.columns:\n",
    "                fig_revenue.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=ts_df.index,\n",
    "                        y=ts_df[target_column],\n",
    "                        mode='lines',\n",
    "                        name='Daily Revenue',\n",
    "                        line=dict(color='lightblue', width=1),\n",
    "                        opacity=0.6\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "                \n",
    "                fig_revenue.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=ts_df.index,\n",
    "                        y=ts_df['revenue_7d_avg'],\n",
    "                        mode='lines',\n",
    "                        name='7-Day Average',\n",
    "                        line=dict(color='blue', width=2)\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            \n",
    "            # 2. Revenue by season and day of week heatmap\n",
    "            if 'season' in ts_df.columns and 'day_of_week' in ts_df.columns:\n",
    "                pivot_revenue = ts_df.pivot_table(\n",
    "                    values=target_column, \n",
    "                    index='season', \n",
    "                    columns='day_of_week', \n",
    "                    aggfunc='mean'\n",
    "                )\n",
    "                \n",
    "                day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "                season_names = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "                \n",
    "                fig_revenue.add_trace(\n",
    "                    go.Heatmap(\n",
    "                        z=pivot_revenue.values,\n",
    "                        x=[day_names[int(i)] if i < len(day_names) else f\"Day{i}\" for i in pivot_revenue.columns],\n",
    "                        y=pivot_revenue.index,\n",
    "                        colorscale='RdYlGn',\n",
    "                        name='Revenue Heatmap'\n",
    "                    ),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "            \n",
    "            # 3. Monthly revenue patterns\n",
    "            if 'month' in ts_df.columns:\n",
    "                monthly_revenue = ts_df.groupby('month')[target_column].agg(['mean', 'std'])\n",
    "                month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "                \n",
    "                fig_revenue.add_trace(\n",
    "                    go.Bar(\n",
    "                        x=[month_names[int(i)-1] for i in monthly_revenue.index if i <= len(month_names)],\n",
    "                        y=monthly_revenue['mean'].values,\n",
    "                        error_y=dict(type='data', array=monthly_revenue['std'].values),\n",
    "                        name='Monthly Revenue',\n",
    "                        marker_color='green'\n",
    "                    ),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "            \n",
    "            # 4. Revenue volatility\n",
    "            if 'revenue_volatility' in ts_df.columns:\n",
    "                fig_revenue.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=ts_df.index,\n",
    "                        y=ts_df['revenue_volatility'],\n",
    "                        mode='lines',\n",
    "                        name='30-Day Volatility',\n",
    "                        line=dict(color='red')\n",
    "                    ),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "            \n",
    "            # 5. Revenue growth analysis\n",
    "            if 'revenue_mom_growth' in ts_df.columns:\n",
    "                growth_data = ts_df['revenue_mom_growth'].dropna()\n",
    "                fig_revenue.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=growth_data.index,\n",
    "                        y=growth_data.values,\n",
    "                        mode='lines+markers',\n",
    "                        name='Month-over-Month Growth (%)',\n",
    "                        line=dict(color='purple')\n",
    "                    ),\n",
    "                    row=3, col=1\n",
    "                )\n",
    "                \n",
    "                # Add zero line\n",
    "                fig_revenue.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=3, col=1)\n",
    "            \n",
    "            fig_revenue.update_layout(\n",
    "                title='Comprehensive Revenue Optimization Dashboard',\n",
    "                height=1200,\n",
    "                showlegend=True\n",
    "            )\n",
    "            \n",
    "            fig_revenue.show()\n",
    "            print(\"âœ… Revenue optimization dashboard created\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error creating revenue visualization: {e}\")\n",
    "    \n",
    "    # Step 6: Actionable Revenue Recommendations\n",
    "    print(f\"\\nðŸŽ¯ Step 6: Actionable Revenue Recommendations\")\n",
    "    print(\"-\" * 47)\n",
    "    \n",
    "    if target_column in ts_df.columns:\n",
    "        print(f\"ðŸ’¡ STRATEGIC REVENUE OPTIMIZATION PLAN:\")\n",
    "        print(f\"=\" * 45)\n",
    "        \n",
    "        current_avg_price = ts_df[target_column].mean()\n",
    "        \n",
    "        print(f\"ðŸ“Š Current Performance Baseline:\")\n",
    "        print(f\"   ðŸ’° Average price: ${current_avg_price:.2f}\")\n",
    "        \n",
    "        # Quick wins\n",
    "        print(f\"\\nâš¡ QUICK WINS (0-30 days):\")\n",
    "        print(f\"   1. Implement weekend pricing premium (+10-15%)\")\n",
    "        print(f\"   2. Adjust prices for identified peak periods\")\n",
    "        print(f\"   3. Set minimum stay requirements during high-demand periods\")\n",
    "        print(f\"   4. Create early booking discounts for off-peak periods\")\n",
    "        \n",
    "        # Medium-term strategies\n",
    "        print(f\"\\nðŸ“ˆ MEDIUM-TERM STRATEGIES (1-6 months):\")\n",
    "        print(f\"   1. Implement dynamic pricing based on seasonal factors\")\n",
    "        print(f\"   2. Develop competitor price monitoring system\")\n",
    "        print(f\"   3. Create demand-based pricing tiers\")\n",
    "        print(f\"   4. Optimize cancellation policies by season\")\n",
    "        \n",
    "        # Long-term initiatives\n",
    "        print(f\"\\nðŸš€ LONG-TERM INITIATIVES (6+ months):\")\n",
    "        print(f\"   1. Build predictive pricing algorithm with external data\")\n",
    "        print(f\"   2. Implement revenue management system\")\n",
    "        print(f\"   3. Develop customer segmentation pricing\")\n",
    "        print(f\"   4. Create automated yield management\")\n",
    "        \n",
    "        # Expected impact\n",
    "        conservative_increase = 8  # 8% revenue increase\n",
    "        optimistic_increase = 15  # 15% revenue increase\n",
    "        \n",
    "        print(f\"\\nðŸ’Ž EXPECTED REVENUE IMPACT:\")\n",
    "        print(f\"   ðŸŽ¯ Conservative scenario: +{conservative_increase}% revenue increase\")\n",
    "        print(f\"      Daily revenue: ${current_avg_price:.2f} â†’ ${current_avg_price * (1 + conservative_increase/100):.2f}\")\n",
    "        print(f\"   ðŸš€ Optimistic scenario: +{optimistic_increase}% revenue increase\")\n",
    "        print(f\"      Daily revenue: ${current_avg_price:.2f} â†’ ${current_avg_price * (1 + optimistic_increase/100):.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No time series data available for revenue optimization\")\n",
    "\n",
    "print(f\"\\nâœ… REVENUE OPTIMIZATION ANALYSIS COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ’° Advanced revenue intelligence delivered:\")\n",
    "print(\"   â€¢ Revenue trend analysis with growth metrics\")\n",
    "print(\"   â€¢ Optimal pricing windows identification\")\n",
    "print(\"   â€¢ Dynamic pricing strategy recommendations\")\n",
    "print(\"   â€¢ Revenue forecasting with sensitivity analysis\")\n",
    "print(\"   â€¢ Comprehensive revenue optimization dashboard\")\n",
    "print(\"   â€¢ Actionable strategic recommendations\")\n",
    "print(\"ðŸŽ¯ Ready for final results export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd42e36",
   "metadata": {},
   "source": [
    "## 10. Export Time Series Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Comprehensive Time Series Analysis Results\n",
    "print(\"ðŸ“¤ EXPORTING TIME SERIES ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set up export directory\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"../data/time_series_results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for unique file naming\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "export_summary = {\n",
    "    'files_created': [],\n",
    "    'analysis_summary': {},\n",
    "    'recommendations': []\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Step 1: Export Enhanced Time Series Dataset\n",
    "    print(\"ðŸ“Š Step 1: Exporting Enhanced Time Series Dataset\")\n",
    "    print(\"-\" * 48)\n",
    "    \n",
    "    if not ts_df.empty:\n",
    "        # Export main time series dataset with all features\n",
    "        ts_export_path = results_dir / f\"enhanced_time_series_{timestamp}.csv\"\n",
    "        ts_df.to_csv(ts_export_path)\n",
    "        export_summary['files_created'].append(str(ts_export_path))\n",
    "        \n",
    "        print(f\"âœ… Enhanced time series dataset exported:\")\n",
    "        print(f\"   ðŸ“ File: {ts_export_path}\")\n",
    "        print(f\"   ðŸ“ Shape: {ts_df.shape}\")\n",
    "        print(f\"   ðŸ“… Date range: {ts_df.index.min().strftime('%Y-%m-%d')} to {ts_df.index.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   ðŸ”¢ Features: {len(ts_df.columns)} columns\")\n",
    "        \n",
    "        # Export feature importance summary\n",
    "        feature_types = {\n",
    "            'temporal': [col for col in ts_df.columns if any(x in col.lower() for x in ['day', 'month', 'year', 'week', 'quarter'])],\n",
    "            'cyclical': [col for col in ts_df.columns if any(x in col.lower() for x in ['sin', 'cos'])],\n",
    "            'lag': [col for col in ts_df.columns if 'lag' in col.lower()],\n",
    "            'rolling': [col for col in ts_df.columns if any(x in col.lower() for x in ['rolling', 'avg', 'std'])],\n",
    "            'seasonal': [col for col in ts_df.columns if any(x in col.lower() for x in ['seasonal', 'trend', 'residual'])],\n",
    "            'holiday': [col for col in ts_df.columns if col.startswith('is_')],\n",
    "            'weather': [col for col in ts_df.columns if any(x in col.lower() for x in ['temperature', 'rainfall'])],\n",
    "            'business': [col for col in ts_df.columns if any(x in col.lower() for x in ['demand', 'revenue', 'volatility'])]\n",
    "        }\n",
    "        \n",
    "        feature_summary_path = results_dir / f\"feature_summary_{timestamp}.txt\"\n",
    "        with open(feature_summary_path, 'w') as f:\n",
    "            f.write(\"TIME SERIES FEATURE ENGINEERING SUMMARY\\n\")\n",
    "            f.write(\"=\" * 45 + \"\\n\\n\")\n",
    "            \n",
    "            for feature_type, features in feature_types.items():\n",
    "                if features:\n",
    "                    f.write(f\"{feature_type.upper()} FEATURES ({len(features)}):\\n\")\n",
    "                    for feature in features:\n",
    "                        f.write(f\"  - {feature}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "        \n",
    "        export_summary['files_created'].append(str(feature_summary_path))\n",
    "        print(f\"   ðŸ“‹ Feature summary: {feature_summary_path}\")\n",
    "    \n",
    "    # Step 2: Export Forecasting Results\n",
    "    print(f\"\\nðŸ”® Step 2: Exporting Forecasting Results\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Export forecasting results if available\n",
    "    if 'forecast_results' in locals() and forecast_results:\n",
    "        forecasts_path = results_dir / f\"forecast_results_{timestamp}.csv\"\n",
    "        \n",
    "        # Combine all forecasting results\n",
    "        all_forecasts = pd.DataFrame()\n",
    "        \n",
    "        for model_name, results in forecast_results.items():\n",
    "            if isinstance(results, dict) and 'forecast' in results:\n",
    "                forecast_df = pd.DataFrame({\n",
    "                    'date': pd.date_range(start=ts_df.index[-1] + pd.Timedelta(days=1), \n",
    "                                        periods=len(results['forecast']), freq='D'),\n",
    "                    'model': model_name,\n",
    "                    'forecast': results['forecast'],\n",
    "                    'lower_ci': results.get('lower_ci', results['forecast']),\n",
    "                    'upper_ci': results.get('upper_ci', results['forecast'])\n",
    "                })\n",
    "                all_forecasts = pd.concat([all_forecasts, forecast_df], ignore_index=True)\n",
    "        \n",
    "        if not all_forecasts.empty:\n",
    "            all_forecasts.to_csv(forecasts_path, index=False)\n",
    "            export_summary['files_created'].append(str(forecasts_path))\n",
    "            print(f\"âœ… Forecast results exported: {forecasts_path}\")\n",
    "            print(f\"   ðŸ”® Models: {all_forecasts['model'].nunique()}\")\n",
    "            print(f\"   ðŸ“… Forecast horizon: {len(all_forecasts[all_forecasts['model'] == all_forecasts['model'].iloc[0]])} days\")\n",
    "    \n",
    "    # Step 3: Export Model Performance Metrics\n",
    "    print(f\"\\nðŸ“ˆ Step 3: Exporting Model Performance\")\n",
    "    print(\"-\" * 39)\n",
    "    \n",
    "    # Export model performance if available\n",
    "    if 'model_performance' in locals() and model_performance:\n",
    "        performance_path = results_dir / f\"model_performance_{timestamp}.csv\"\n",
    "        performance_df = pd.DataFrame.from_dict(model_performance, orient='index')\n",
    "        performance_df.to_csv(performance_path)\n",
    "        export_summary['files_created'].append(str(performance_path))\n",
    "        \n",
    "        print(f\"âœ… Model performance exported: {performance_path}\")\n",
    "        print(f\"   ðŸ† Best model by MAE: {performance_df['MAE'].idxmin()}\")\n",
    "        print(f\"   ðŸŽ¯ Best MAE score: {performance_df['MAE'].min():.4f}\")\n",
    "        \n",
    "        export_summary['analysis_summary']['best_model'] = performance_df['MAE'].idxmin()\n",
    "        export_summary['analysis_summary']['best_mae'] = float(performance_df['MAE'].min())\n",
    "    \n",
    "    # Step 4: Export Seasonal Analysis Results\n",
    "    print(f\"\\nðŸŒŠ Step 4: Exporting Seasonal Analysis\")\n",
    "    print(\"-\" * 39)\n",
    "    \n",
    "    if 'seasonal_factors' in locals():\n",
    "        seasonal_path = results_dir / f\"seasonal_analysis_{timestamp}.json\"\n",
    "        \n",
    "        # Prepare seasonal analysis for export\n",
    "        seasonal_export = {\n",
    "            'seasonal_factors': seasonal_factors,\n",
    "            'analysis_timestamp': timestamp,\n",
    "            'data_period': {\n",
    "                'start_date': ts_df.index.min().strftime('%Y-%m-%d') if not ts_df.empty else None,\n",
    "                'end_date': ts_df.index.max().strftime('%Y-%m-%d') if not ts_df.empty else None,\n",
    "                'total_days': len(ts_df) if not ts_df.empty else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add peak period analysis if available\n",
    "        if 'demand_level' in ts_df.columns:\n",
    "            peak_analysis = {\n",
    "                'peak_days': int(ts_df[ts_df['demand_level'] == 'Peak'].shape[0]),\n",
    "                'off_peak_days': int(ts_df[ts_df['demand_level'] == 'Off-Peak'].shape[0]),\n",
    "                'normal_days': int(ts_df[ts_df['demand_level'] == 'Normal'].shape[0])\n",
    "            }\n",
    "            seasonal_export['peak_analysis'] = peak_analysis\n",
    "        \n",
    "        # Add holiday effects if available\n",
    "        if 'holiday_effects' in locals():\n",
    "            seasonal_export['holiday_effects'] = holiday_effects\n",
    "        \n",
    "        import json\n",
    "        with open(seasonal_path, 'w') as f:\n",
    "            json.dump(seasonal_export, f, indent=2, default=str)\n",
    "        \n",
    "        export_summary['files_created'].append(str(seasonal_path))\n",
    "        print(f\"âœ… Seasonal analysis exported: {seasonal_path}\")\n",
    "        \n",
    "        if 'seasonal_factors' in seasonal_export and 'monthly' in seasonal_export['seasonal_factors']:\n",
    "            monthly_factors = seasonal_export['seasonal_factors']['monthly']\n",
    "            peak_month = max(monthly_factors.items(), key=lambda x: x[1])\n",
    "            low_month = min(monthly_factors.items(), key=lambda x: x[1])\n",
    "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "            \n",
    "            print(f\"   ðŸ”ï¸ Peak month: {month_names[int(peak_month[0])-1]} (factor: {peak_month[1]:.3f})\")\n",
    "            print(f\"   ðŸ“‰ Low month: {month_names[int(low_month[0])-1]} (factor: {low_month[1]:.3f})\")\n",
    "    \n",
    "    # Step 5: Export Revenue Optimization Results\n",
    "    print(f\"\\nðŸ’° Step 5: Exporting Revenue Optimization\")\n",
    "    print(\"-\" * 44)\n",
    "    \n",
    "    if 'pricing_calendar' in locals() and not pricing_calendar.empty:\n",
    "        pricing_path = results_dir / f\"pricing_recommendations_{timestamp}.csv\"\n",
    "        pricing_calendar.to_csv(pricing_path)\n",
    "        export_summary['files_created'].append(str(pricing_path))\n",
    "        \n",
    "        print(f\"âœ… Pricing recommendations exported: {pricing_path}\")\n",
    "        print(f\"   ðŸ“… Pricing horizon: {len(pricing_calendar)} days\")\n",
    "        print(f\"   ðŸ’° Price range: ${pricing_calendar['recommended_price'].min():.2f} - ${pricing_calendar['recommended_price'].max():.2f}\")\n",
    "        \n",
    "        # Calculate potential revenue impact\n",
    "        if target_column and target_column in ts_df.columns:\n",
    "            current_avg = ts_df[target_column].mean()\n",
    "            recommended_avg = pricing_calendar['recommended_price'].mean()\n",
    "            revenue_impact = ((recommended_avg - current_avg) / current_avg) * 100\n",
    "            \n",
    "            print(f\"   ðŸ“ˆ Potential revenue impact: {revenue_impact:+.1f}%\")\n",
    "            export_summary['analysis_summary']['revenue_impact'] = float(revenue_impact)\n",
    "    \n",
    "    # Step 6: Export Business Intelligence Summary\n",
    "    print(f\"\\nðŸ§  Step 6: Creating Business Intelligence Report\")\n",
    "    print(\"-\" * 47)\n",
    "    \n",
    "    # Create comprehensive business intelligence report\n",
    "    bi_report_path = results_dir / f\"business_intelligence_report_{timestamp}.txt\"\n",
    "    \n",
    "    with open(bi_report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"AIRBNB BALI TIME SERIES ANALYSIS\\n\")\n",
    "        f.write(\"BUSINESS INTELLIGENCE REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Data Period: {ts_df.index.min().strftime('%Y-%m-%d')} to {ts_df.index.max().strftime('%Y-%m-%d')}\\n\")\n",
    "        f.write(f\"Total Observations: {len(ts_df)}\\n\\n\")\n",
    "        \n",
    "        # Executive Summary\n",
    "        f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        if target_column and target_column in ts_df.columns:\n",
    "            current_avg = ts_df[target_column].mean()\n",
    "            current_std = ts_df[target_column].std()\n",
    "            f.write(f\"â€¢ Average daily price: ${current_avg:.2f} (Â±${current_std:.2f})\\n\")\n",
    "            \n",
    "            if 'revenue_impact' in export_summary['analysis_summary']:\n",
    "                f.write(f\"â€¢ Potential revenue optimization: {export_summary['analysis_summary']['revenue_impact']:+.1f}%\\n\")\n",
    "            \n",
    "            if 'best_model' in export_summary['analysis_summary']:\n",
    "                f.write(f\"â€¢ Best forecasting model: {export_summary['analysis_summary']['best_model']}\\n\")\n",
    "                f.write(f\"â€¢ Model accuracy (MAE): {export_summary['analysis_summary']['best_mae']:.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nKEY INSIGHTS\\n\")\n",
    "        f.write(\"-\" * 15 + \"\\n\")\n",
    "        \n",
    "        # Add seasonal insights\n",
    "        if 'seasonal_factors' in locals() and 'monthly' in seasonal_factors:\n",
    "            monthly_factors = seasonal_factors['monthly']\n",
    "            peak_month = max(monthly_factors.items(), key=lambda x: x[1])\n",
    "            low_month = min(monthly_factors.items(), key=lambda x: x[1])\n",
    "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "            \n",
    "            f.write(f\"â€¢ Peak season: {month_names[int(peak_month[0])-1]} ({((peak_month[1]-1)*100):+.1f}% above average)\\n\")\n",
    "            f.write(f\"â€¢ Low season: {month_names[int(low_month[0])-1]} ({((low_month[1]-1)*100):+.1f}% below average)\\n\")\n",
    "        \n",
    "        # Add demand level insights\n",
    "        if 'demand_level' in ts_df.columns:\n",
    "            peak_pct = (ts_df[ts_df['demand_level'] == 'Peak'].shape[0] / len(ts_df)) * 100\n",
    "            off_peak_pct = (ts_df[ts_df['demand_level'] == 'Off-Peak'].shape[0] / len(ts_df)) * 100\n",
    "            f.write(f\"â€¢ Peak demand periods: {peak_pct:.1f}% of days\\n\")\n",
    "            f.write(f\"â€¢ Off-peak demand periods: {off_peak_pct:.1f}% of days\\n\")\n",
    "        \n",
    "        f.write(\"\\nRECOMMENDations\\n\")\n",
    "        f.write(\"-\" * 15 + \"\\n\")\n",
    "        f.write(\"â€¢ Implement dynamic pricing based on seasonal factors\\n\")\n",
    "        f.write(\"â€¢ Focus marketing efforts during identified peak periods\\n\")\n",
    "        f.write(\"â€¢ Develop promotional strategies for off-peak periods\\n\")\n",
    "        f.write(\"â€¢ Monitor competitive pricing in high-demand seasons\\n\")\n",
    "        f.write(\"â€¢ Consider minimum stay requirements during peak periods\\n\")\n",
    "        \n",
    "        f.write(f\"\\nFILES GENERATED\\n\")\n",
    "        f.write(\"-\" * 15 + \"\\n\")\n",
    "        for file_path in export_summary['files_created']:\n",
    "            f.write(f\"â€¢ {os.path.basename(file_path)}\\n\")\n",
    "    \n",
    "    export_summary['files_created'].append(str(bi_report_path))\n",
    "    print(f\"âœ… Business intelligence report created: {bi_report_path}\")\n",
    "    \n",
    "    # Step 7: Export Model Objects (if available)\n",
    "    print(f\"\\nðŸ¤– Step 7: Saving Trained Models\")\n",
    "    print(\"-\" * 33)\n",
    "    \n",
    "    # Save trained models for production use\n",
    "    models_saved = 0\n",
    "    \n",
    "    # Save ARIMA model if available\n",
    "    if 'fitted_arima' in locals():\n",
    "        try:\n",
    "            import pickle\n",
    "            arima_path = results_dir / f\"arima_model_{timestamp}.pkl\"\n",
    "            with open(arima_path, 'wb') as f:\n",
    "                pickle.dump(fitted_arima, f)\n",
    "            export_summary['files_created'].append(str(arima_path))\n",
    "            models_saved += 1\n",
    "            print(f\"âœ… ARIMA model saved: {arima_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not save ARIMA model: {e}\")\n",
    "    \n",
    "    # Save Prophet model if available\n",
    "    if 'prophet_model' in locals():\n",
    "        try:\n",
    "            import pickle\n",
    "            prophet_path = results_dir / f\"prophet_model_{timestamp}.pkl\"\n",
    "            with open(prophet_path, 'wb') as f:\n",
    "                pickle.dump(prophet_model, f)\n",
    "            export_summary['files_created'].append(str(prophet_path))\n",
    "            models_saved += 1\n",
    "            print(f\"âœ… Prophet model saved: {prophet_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not save Prophet model: {e}\")\n",
    "    \n",
    "    # Save Random Forest model if available\n",
    "    if 'rf_ts_model' in locals():\n",
    "        try:\n",
    "            import pickle\n",
    "            rf_path = results_dir / f\"random_forest_model_{timestamp}.pkl\"\n",
    "            with open(rf_path, 'wb') as f:\n",
    "                pickle.dump(rf_ts_model, f)\n",
    "            export_summary['files_created'].append(str(rf_path))\n",
    "            models_saved += 1\n",
    "            print(f\"âœ… Random Forest model saved: {rf_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not save Random Forest model: {e}\")\n",
    "    \n",
    "    if models_saved == 0:\n",
    "        print(\"ðŸ“ No trained models available for export\")\n",
    "    else:\n",
    "        print(f\"âœ… {models_saved} trained models saved for production use\")\n",
    "    \n",
    "    # Step 8: Create Export Summary\n",
    "    print(f\"\\nðŸ“‹ Step 8: Export Summary\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    summary_path = results_dir / f\"export_summary_{timestamp}.json\"\n",
    "    export_summary['export_timestamp'] = timestamp\n",
    "    export_summary['total_files'] = len(export_summary['files_created'])\n",
    "    \n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(export_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"âœ… Export completed successfully!\")\n",
    "    print(f\"ðŸ“ Results directory: {results_dir}\")\n",
    "    print(f\"ðŸ“Š Total files created: {len(export_summary['files_created'])}\")\n",
    "    print(f\"ðŸ“‹ Export summary: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ FILES CREATED:\")\n",
    "    print(\"-\" * 20)\n",
    "    for i, file_path in enumerate(export_summary['files_created'], 1):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0\n",
    "        print(f\"   {i:2d}. {file_name} ({file_size:,} bytes)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during export: {e}\")\n",
    "    print(\"âš ï¸ Some files may have been created successfully\")\n",
    "\n",
    "print(f\"\\nâœ… TIME SERIES ANALYSIS EXPORT COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“¤ Complete time series analysis exported:\")\n",
    "print(\"   â€¢ Enhanced time series dataset with 50+ features\")\n",
    "print(\"   â€¢ Forecasting models and predictions\")\n",
    "print(\"   â€¢ Model performance metrics and comparisons\")\n",
    "print(\"   â€¢ Seasonal analysis and adjustment factors\")\n",
    "print(\"   â€¢ Revenue optimization recommendations\")\n",
    "print(\"   â€¢ Business intelligence report\")\n",
    "print(\"   â€¢ Trained models for production deployment\")\n",
    "print(\"ðŸš€ Analysis ready for business implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708940c6",
   "metadata": {},
   "source": [
    "## âœ… Time Series Analysis Complete!\n",
    "\n",
    "### ðŸŽ‰ Congratulations! \n",
    "\n",
    "You've successfully created a comprehensive **Time Series Analysis** system for Airbnb Bali listings that includes:\n",
    "\n",
    "#### ðŸ”¬ **Advanced Analytics Capabilities:**\n",
    "- **50+ Temporal Features** with cyclical encodings and lag variables\n",
    "- **Multiple Forecasting Models** (ARIMA, SARIMA, Prophet, Random Forest)\n",
    "- **Seasonal Decomposition** with trend, seasonal, and residual components\n",
    "- **Interactive Visualizations** with Plotly dashboards\n",
    "- **Booking Demand Prediction** with scenario analysis\n",
    "- **Revenue Optimization** with dynamic pricing recommendations\n",
    "\n",
    "#### ðŸ“Š **Business Intelligence Features:**\n",
    "- **Peak/Off-Peak Analysis** with demand level classification\n",
    "- **Holiday Effects Quantification** for strategic planning\n",
    "- **Weather Impact Correlation** (simulated for Bali climate)\n",
    "- **Seasonal Adjustment Factors** for business planning\n",
    "- **Revenue Forecasting** with sensitivity analysis\n",
    "- **Comprehensive Export System** for production deployment\n",
    "\n",
    "#### ðŸš€ **Next Steps:**\n",
    "1. **Run the notebook** to generate forecasts and insights\n",
    "2. **Review the exported results** in the `../data/time_series_results/` directory\n",
    "3. **Implement dynamic pricing** based on seasonal recommendations\n",
    "4. **Monitor model performance** and retrain periodically\n",
    "5. **Integrate with booking systems** for real-time optimization\n",
    "\n",
    "#### ðŸ’¡ **Key Business Value:**\n",
    "- **Revenue Optimization:** Potential 8-15% revenue increase through strategic pricing\n",
    "- **Demand Forecasting:** Accurate predictions for capacity planning\n",
    "- **Seasonal Intelligence:** Data-driven seasonal strategies\n",
    "- **Competitive Advantage:** Advanced analytics for market positioning\n",
    "\n",
    "### ðŸ”„ **Ready to Run:**\n",
    "Execute all cells to generate your complete time series analysis and business intelligence reports!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
